{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ0ImPmiK8zP"
      },
      "source": [
        "Setup/installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odNdHNstK7wk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "!pip -q install folium pyproj scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import folium\n",
        "from sklearn.neighbors import BallTree\n",
        "from pyproj import Transformer\n",
        "from math import sin, cos, asin, sqrt\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B-raDHoLF62"
      },
      "source": [
        "configurations and paramaters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5TMA9GoLJt2"
      },
      "outputs": [],
      "source": [
        "# --- FILE PATHS (adjust if needed) ---\n",
        "STREETLIGHT_CSV = \"/content/drive/My Drive/Street_Lights.csv\"\n",
        "CRASH_CSV       = \"/content/drive/My Drive/Crashes_in_DC.csv\"\n",
        "\n",
        "# --- PARAMETERS ---\n",
        "START_DATE = \"2020-01-01\"\n",
        "END_DATE   = \"2025-04-30\"\n",
        "NIGHT_HOURS = list(range(0, 6)) + list(range(20, 24))\n",
        "\n",
        "WITHIN_THRESHOLD_M = 30\n",
        "FAR_THRESHOLD_M    = 30\n",
        "\n",
        "CLUSTER_RADIUS_M      = 30\n",
        "MIN_CLUSTER_POINTS    = 5\n",
        "DRAW_SIZE_CIRCLES     = True\n",
        "RADIUS_TOLERANCE_M    = 1e-6\n",
        "\n",
        "EARTH_RADIUS_M = 6_371_000.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMYJxAWdLf3v"
      },
      "source": [
        "add helpers/check longitude and latitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdkRyHkNLfIl"
      },
      "outputs": [],
      "source": [
        "def looks_projected(df, lat_col=\"LATITUDE\", lon_col=\"LONGITUDE\"):\n",
        "    lat_max = df[lat_col].abs().max()\n",
        "    lon_max = df[lon_col].abs().max()\n",
        "    return (lat_max > 90) or (lon_max > 180) or (lat_max > 1000) or (lon_max > 1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPJ1ZYrnL_Tk"
      },
      "source": [
        "Lode and clean streetlights\n",
        "\n",
        "Load raw data\n",
        "‚Üí Read the streetlight CSV into a pandas DataFrame.\n",
        "\n",
        "Normalize column names\n",
        "‚Üí Strip spaces and uppercase all headers for consistency.\n",
        "\n",
        " Standardize coordinate columns\n",
        "‚Üí Rename X and Y to LONGITUDE and LATITUDE if needed.\n",
        "\n",
        " Drop invalid rows\n",
        "‚Üí Remove any rows missing LATITUDE or LONGITUDE.\n",
        "\n",
        " Ensure numeric coordinates\n",
        "‚Üí Convert coordinates to floats; drop anything unconvertible.\n",
        "\n",
        " Detect if data is projected (in meters)\n",
        "‚Üí Use looks_projected() to check if coordinates need conversion.\n",
        "\n",
        " Reproject if needed (EPSG:3857 ‚Üí EPSG:4326)\n",
        "‚Üí Transform projected meters into standard lat/lon (WGS84).\n",
        "\n",
        " Remove duplicates\n",
        "‚Üí Drop duplicate lat/lon points to prevent map clutter and analysis bias.\n",
        "\n",
        " Final sanity check\n",
        "‚Üí Raise an error if the dataset ends up empty after cleaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r67Q3wFMLmJl"
      },
      "outputs": [],
      "source": [
        "from pyproj import Transformer\n",
        "import numpy as np\n",
        "\n",
        "streetlights = pd.read_csv(STREETLIGHT_CSV, low_memory=False)\n",
        "\n",
        "# Normalize column names\n",
        "orig_cols = streetlights.columns.tolist()\n",
        "streetlights.rename(columns={c: c.strip().upper() for c in streetlights.columns}, inplace=True)\n",
        "print(\"Columns (after normalize):\\n\", streetlights.columns.tolist())\n",
        "if orig_cols != streetlights.columns.tolist():\n",
        "    print(\"‚öôÔ∏è Renamed some columns to upper/trimmed whitespace.\")\n",
        "\n",
        "# --- Helpers -----------------------------------------------------------------\n",
        "def has_latlon(df):\n",
        "    return {\"LAT\", \"LON\"}.issubset(df.columns) or {\"LATITUDE\", \"LONGITUDE\"}.issubset(df.columns)\n",
        "\n",
        "def detect_xy_crs(x, y):\n",
        "    \"\"\"\n",
        "    Heuristically detect the CRS of projected X/Y columns.\n",
        "\n",
        "    Returns: (\"EPSG:xxxx\", reason_string)\n",
        "    \"\"\"\n",
        "    x = pd.to_numeric(x, errors=\"coerce\")\n",
        "    y = pd.to_numeric(y, errors=\"coerce\")\n",
        "    # Drop NaNs for stats\n",
        "    xs = x[np.isfinite(x)]\n",
        "    ys = y[np.isfinite(y)]\n",
        "    if xs.empty or ys.empty:\n",
        "        return None, \"insufficient finite X/Y values\"\n",
        "\n",
        "    xmin, xmax = xs.min(), xs.max()\n",
        "    ymin, ymax = ys.min(), ys.max()\n",
        "\n",
        "    # If values look like degrees (already lon/lat)\n",
        "    if (-180 <= xmin <= 180) and (-180 <= xmax <= 180) and (-90 <= ymin <= 90) and (-90 <= ymax <= 90):\n",
        "        return \"EPSG:4326\", \"values appear to be degrees\"\n",
        "\n",
        "    # Web Mercator meters range ~ ¬±20,037,508\n",
        "    if all(abs(v) <= 2.1e7 for v in [xmin, xmax, ymin, ymax]):\n",
        "        # Typical DC Web Mercator should be around x ~ -8.6e6, y ~ 4.6e6\n",
        "        return \"EPSG:3857\", \"values within Web Mercator meter range\"\n",
        "\n",
        "    # Maryland StatePlane (NAD83) US-ft (DC commonly uses 2248)\n",
        "    # Typical magnitudes in hundreds of thousands to a few million feet\n",
        "    if all(1e4 <= abs(v) <= 1e7 for v in [xmin, xmax, ymin, ymax]):\n",
        "        return \"EPSG:2248\", \"values look like StatePlane Maryland (US-ft)\"\n",
        "\n",
        "    # NAD83 / Maryland meters variant (less common for DDOT tabular): EPSG:26985\n",
        "    if all(1e3 <= abs(v) <= 1e6 for v in [xmin, xmax, ymin, ymax]):\n",
        "        return \"EPSG:26985\", \"values look like StatePlane Maryland (meters)\"\n",
        "\n",
        "    return None, \"unable to infer CRS from value ranges\"\n",
        "\n",
        "def transform_xy_to_wgs84(x, y, src_epsg):\n",
        "    tf = Transformer.from_crs(src_epsg, \"EPSG:4326\", always_xy=True)\n",
        "    lon, lat = tf.transform(x.to_numpy(), y.to_numpy())\n",
        "    return pd.Series(lat), pd.Series(lon)\n",
        "\n",
        "# --- Coordinate column handling ---------------------------------------------\n",
        "# Prefer existing LAT/LON if present\n",
        "if has_latlon(streetlights):\n",
        "    # Normalize to LATITUDE/LONGITUDE names\n",
        "    if {\"LAT\", \"LON\"}.issubset(streetlights.columns):\n",
        "        streetlights.rename(columns={\"LAT\": \"LATITUDE\", \"LON\": \"LONGITUDE\"}, inplace=True)\n",
        "    # If already LATITUDE/LONGITUDE, leave as-is\n",
        "    print(\"‚úÖ Using existing LATITUDE/LONGITUDE columns.\")\n",
        "\n",
        "elif {\"X\", \"Y\"}.issubset(streetlights.columns):\n",
        "    # Try to detect CRS of X/Y\n",
        "    src_epsg, reason = detect_xy_crs(streetlights[\"X\"], streetlights[\"Y\"])\n",
        "    if src_epsg is None:\n",
        "        raise ValueError(f\"‚ùå Could not infer CRS for X/Y ‚Äî {reason}. Provide the correct EPSG code.\")\n",
        "\n",
        "    print(f\"‚ÜîÔ∏è Converting X/Y ‚Üí WGS84 (EPSG:4326) from {src_epsg} ({reason}).\")\n",
        "\n",
        "    # Transform\n",
        "    lat_wgs, lon_wgs = transform_xy_to_wgs84(streetlights[\"X\"], streetlights[\"Y\"], src_epsg)\n",
        "    streetlights[\"LATITUDE\"] = pd.to_numeric(lat_wgs, errors=\"coerce\")\n",
        "    streetlights[\"LONGITUDE\"] = pd.to_numeric(lon_wgs, errors=\"coerce\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"‚ùå Could not find coordinate columns (LAT/LON or X/Y).\")\n",
        "\n",
        "# --- Diagnostics: show a few sample coordinates -----------------------------\n",
        "print(\"\\n=== STREETLIGHT COORDINATE SAMPLES ===\")\n",
        "print(streetlights[[\"LATITUDE\", \"LONGITUDE\"]].head(5))\n",
        "\n",
        "# --- Clean up ----------------------------------------------------------------\n",
        "# Drop bad coords\n",
        "streetlights = streetlights.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"]).copy()\n",
        "streetlights[\"LATITUDE\"]  = pd.to_numeric(streetlights[\"LATITUDE\"], errors=\"coerce\")\n",
        "streetlights[\"LONGITUDE\"] = pd.to_numeric(streetlights[\"LONGITUDE\"], errors=\"coerce\")\n",
        "streetlights = streetlights.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"]).copy()\n",
        "\n",
        "# Fix common sign issues: DC longitudes must be negative (~ -77)\n",
        "if streetlights[\"LONGITUDE\"].median() > 0:\n",
        "    print(\"üß≠ Detected positive longitudes; flipping sign to negative (W hemisphere).\")\n",
        "    streetlights[\"LONGITUDE\"] = -streetlights[\"LONGITUDE\"].abs()\n",
        "\n",
        "# Remove exact duplicate points\n",
        "streetlights = streetlights.drop_duplicates(subset=[\"LATITUDE\",\"LONGITUDE\"]).reset_index(drop=True)\n",
        "\n",
        "# Only keep DDOT lights that still physically exist (if available)\n",
        "if \"ASSETSTATUS\" in streetlights.columns:\n",
        "    pre_status = len(streetlights)\n",
        "    streetlights = streetlights[\n",
        "        ~streetlights[\"ASSETSTATUS\"].astype(str).str.contains(\"Removed|Decommissioned\", case=False, na=False)\n",
        "    ].copy()\n",
        "    print(f\"Filtered removed/decommissioned assets: {pre_status - len(streetlights)} dropped.\")\n",
        "\n",
        "# Clip to central DC extent\n",
        "LAT_MIN, LAT_MAX = 38.81, 38.995\n",
        "LON_MIN, LON_MAX = -77.12, -76.91\n",
        "\n",
        "pre_bbox = len(streetlights)\n",
        "streetlights = streetlights[\n",
        "    (streetlights[\"LATITUDE\"].between(LAT_MIN, LAT_MAX)) &\n",
        "    (streetlights[\"LONGITUDE\"].between(LON_MIN, LON_MAX))\n",
        "].copy()\n",
        "print(f\"üó∫Ô∏è Filtered streetlights to {len(streetlights):,} within DC bounds (from {pre_bbox:,}).\")\n",
        "\n",
        "# Final sanity checks\n",
        "if streetlights.empty:\n",
        "    raise ValueError(\"‚ùå Streetlight dataset is empty after filtering ‚Äî check coordinate range or filters.\")\n",
        "\n",
        "print(\"\\n=== STREETLIGHT COORDINATE CHECK ===\")\n",
        "print(\"Latitude range:\", streetlights[\"LATITUDE\"].min(), \"‚Üí\", streetlights[\"LATITUDE\"].max())\n",
        "print(\"Longitude range:\", streetlights[\"LONGITUDE\"].min(), \"‚Üí\", streetlights[\"LONGITUDE\"].max())\n",
        "print(\"Total streetlights:\", len(streetlights))\n",
        "print(\"‚úÖ Ready for analysis.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNZuFsEPPCzj"
      },
      "source": [
        "Lode and prepare crashes. Set parameters on the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02gPe8glPCaM"
      },
      "outputs": [],
      "source": [
        "# --- LOAD CRASHES ---\n",
        "import os\n",
        "\n",
        "# üîß Make sure CRASH_CSV actually points to a real file\n",
        "# (handles the common \"My Drive\" vs \"MyDrive\" issue without changing other cells)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "except Exception:\n",
        "    # If not in Colab or already mounted, this just fails quietly\n",
        "    pass\n",
        "\n",
        "if not os.path.exists(CRASH_CSV):\n",
        "    alt_path = CRASH_CSV.replace(\"My Drive\", \"MyDrive\")\n",
        "    if os.path.exists(alt_path):\n",
        "        print(f\"‚ö†Ô∏è CRASH_CSV not found at:\\n  {CRASH_CSV}\")\n",
        "        print(f\"‚úÖ Using alternate path instead:\\n  {alt_path}\")\n",
        "        CRASH_CSV = alt_path\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"‚ùå Could not find crash file at:\\n  {CRASH_CSV}\\n\"\n",
        "            f\"or at:\\n  {alt_path}\\n\"\n",
        "            f\"‚Üí Update CRASH_CSV to the correct path.\"\n",
        "        )\n",
        "\n",
        "print(\"\\nLoading crash data...\")\n",
        "\n",
        "df = pd.read_csv(CRASH_CSV, dtype={\"STREETSEGID\": str}, low_memory=False)\n",
        "\n",
        "df = df.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"]).copy()\n",
        "df[\"LATITUDE\"]  = pd.to_numeric(df[\"LATITUDE\"], errors=\"coerce\")\n",
        "df[\"LONGITUDE\"] = pd.to_numeric(df[\"LONGITUDE\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"]).copy()\n",
        "\n",
        "df[\"FROMDATE\"] = pd.to_datetime(df[\"FROMDATE\"], errors=\"coerce\")\n",
        "df = df[(df[\"FROMDATE\"] >= START_DATE) & (df[\"FROMDATE\"] <= END_DATE)].copy()\n",
        "\n",
        "# üîç --- Check for duplicate CRIMEIDs before aggregation ---\n",
        "if \"CRIMEID\" in df.columns:\n",
        "    dupes = df[df.duplicated(subset=[\"CRIMEID\"], keep=False)]\n",
        "    print(f\"\\nDuplicate CRIMEIDs: {dupes['CRIMEID'].nunique()}\")\n",
        "    if not dupes.empty:\n",
        "        print(dupes[[\"CRIMEID\", \"FROMDATE\", \"LATITUDE\", \"LONGITUDE\"]].head(10))\n",
        "    else:\n",
        "        print(\"‚úÖ No duplicate CRIMEIDs found.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No CRIMEID column found in dataset.\")\n",
        "\n",
        "print(f\"Loaded {len(df)} crash records between {START_DATE} and {END_DATE}.\")\n",
        "\n",
        "# injury columns\n",
        "fatal_cols = [\"FATAL_BICYCLIST\", \"FATAL_DRIVER\", \"FATAL_PEDESTRIAN\", \"FATALPASSENGER\", \"FATALOTHER\"]\n",
        "major_cols = [\"MAJORINJURIES_BICYCLIST\", \"MAJORINJURIES_DRIVER\", \"MAJORINJURIES_PEDESTRIAN\",\n",
        "              \"MAJORINJURIESPASSENGER\", \"MAJORINJURIESOTHER\"]\n",
        "minor_cols = [\"MINORINJURIES_BICYCLIST\", \"MINORINJURIES_DRIVER\", \"MINORINJURIES_PEDESTRIAN\",\n",
        "              \"MINORINJURIESPASSENGER\", \"MINORINJURIESOTHER\"]\n",
        "sev_cols_all = [c for c in fatal_cols + major_cols + minor_cols if c in df.columns]\n",
        "\n",
        "# --- Deduplicate crash records (ID-based + spatial-temporal) ---\n",
        "print(\"\\nDeduplicating crash data...\")\n",
        "\n",
        "CRASH_KEY = next((k for k in [\"CRIMEID\", \"CRASHID\", \"CASE_ID\", \"OBJECTID\", \"CRASH_ID\"] if k in df.columns), None)\n",
        "print(f\"Using crash key: {CRASH_KEY}\")\n",
        "\n",
        "if CRASH_KEY:\n",
        "    pre = len(df)\n",
        "    df = df.sort_values(\"FROMDATE\").drop_duplicates(subset=[CRASH_KEY], keep=\"first\").copy()\n",
        "    print(f\"Removed {pre - len(df)} duplicate {CRASH_KEY} entries (if any).\")\n",
        "\n",
        "# 2Ô∏è‚É£ Spatial‚Äìtemporal dedupe (same spot ¬±15 min)\n",
        "df[\"_lat_r\"] = df[\"LATITUDE\"].round(6)\n",
        "df[\"_lon_r\"] = df[\"LONGITUDE\"].round(6)\n",
        "df[\"_t15\"]   = df[\"FROMDATE\"].dt.floor(\"15min\")\n",
        "\n",
        "pre_dedupe = len(df)\n",
        "df = (\n",
        "    df.sort_values(\"FROMDATE\")\n",
        "      .drop_duplicates(subset=[\"_lat_r\", \"_lon_r\", \"_t15\"], keep=\"first\")\n",
        "      .drop(columns=[\"_lat_r\", \"_lon_r\", \"_t15\"])\n",
        "      .copy()\n",
        ")\n",
        "print(f\"Removed {pre_dedupe - len(df)} near-duplicate records (same location ¬±15 min).\")\n",
        "print(f\"‚úÖ {len(df)} unique crash events remain after combined deduplication.\\n\")\n",
        "\n",
        "# night + known-injury filters AFTER aggregation\n",
        "df[\"HOUR\"] = df[\"FROMDATE\"].dt.hour\n",
        "df = df[df[\"HOUR\"].isin(NIGHT_HOURS)].copy()\n",
        "print(f\"Filtered crashes to nighttime hours: {len(df)} records remain.\")\n",
        "df.drop(columns=[\"HOUR\"], inplace=True)\n",
        "\n",
        "# --- Exclude federal zones (Mall, Capitol, White House) ---\n",
        "federal_zones = [\n",
        "    # National Mall / Capitol Hill\n",
        "    {\"lat_min\": 38.886, \"lat_max\": 38.895, \"lon_min\": -77.04, \"lon_max\": -76.99},\n",
        "    # White House / Ellipse\n",
        "    {\"lat_min\": 38.893, \"lat_max\": 38.899, \"lon_min\": -77.043, \"lon_max\": -77.032},\n",
        "]\n",
        "\n",
        "pre_len = len(df)\n",
        "for z in federal_zones:\n",
        "    mask = (\n",
        "        (df[\"LATITUDE\"].between(z[\"lat_min\"], z[\"lat_max\"])) &\n",
        "        (df[\"LONGITUDE\"].between(z[\"lon_min\"], z[\"lon_max\"]))\n",
        "    )\n",
        "    df = df[~mask]\n",
        "\n",
        "print(f\"Removed {pre_len - len(df)} crashes within approximate federal zones.\")\n",
        "\n",
        "# Optional MAR filter\n",
        "if \"MAR_SCORE\" in df.columns:\n",
        "    pre_mar_count = len(df)\n",
        "    df = df[pd.to_numeric(df[\"MAR_SCORE\"], errors=\"coerce\") >= 100].copy()\n",
        "    print(f\"Applied MAR_SCORE filter >=100: {len(df)} records remain (from {pre_mar_count}).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r73vafHPZqw"
      },
      "source": [
        "calculate distance to streetlight using euclidean distances, no longer haversine metrics, and balltree distancing because dealing with a large data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XPK7bQzD1bx"
      },
      "outputs": [],
      "source": [
        "print(df['LATITUDE'].min(), df['LATITUDE'].max())\n",
        "print(df['LONGITUDE'].min(), df['LONGITUDE'].max())\n",
        "print(df[['LATITUDE','LONGITUDE']].sample(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Box3imDMRNq"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCalculating nearest streetlight distances for crashes...\")\n",
        "\n",
        "from pyproj import Transformer\n",
        "from sklearn.neighbors import BallTree\n",
        "import numpy as np\n",
        "\n",
        "# --- Project lat/lon ‚Üí UTM 18N (meters) for Euclidean distances ---\n",
        "tf_xy = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32618\", always_xy=True)\n",
        "\n",
        "# Streetlights ‚Üí meters\n",
        "sl_Xm, sl_Ym = tf_xy.transform(\n",
        "    streetlights[\"LONGITUDE\"].to_numpy(),\n",
        "    streetlights[\"LATITUDE\"].to_numpy()\n",
        ")\n",
        "SL_XY = np.column_stack([sl_Xm, sl_Ym])\n",
        "\n",
        "# Crashes ‚Üí meters\n",
        "cr_Xm, cr_Ym = tf_xy.transform(\n",
        "    df[\"LONGITUDE\"].to_numpy(),\n",
        "    df[\"LATITUDE\"].to_numpy()\n",
        ")\n",
        "CR_XY = np.column_stack([cr_Xm, cr_Ym])\n",
        "\n",
        "# --- Nearest neighbor using Euclidean metric in meters ---\n",
        "tree = BallTree(SL_XY, metric=\"euclidean\")\n",
        "dist_m, _ = tree.query(CR_XY, k=1)\n",
        "df[\"DIST_TO_LIGHT_M\"] = dist_m.flatten()\n",
        "\n",
        "# --- Remove crashes too close to the streetlight data boundary (in meters) ---\n",
        "# (Prevents \"fake dark zones\" near DC‚ÄìMD border where DDOT data stops)\n",
        "SL_X_MIN, SL_X_MAX = sl_Xm.min(), sl_Xm.max()\n",
        "SL_Y_MIN, SL_Y_MAX = sl_Ym.min(), sl_Ym.max()\n",
        "BUFFER_M = 100  # ~100 meters\n",
        "\n",
        "edge_mask_m = (\n",
        "    (cr_Xm <= SL_X_MIN + BUFFER_M) |\n",
        "    (cr_Xm >= SL_X_MAX - BUFFER_M) |\n",
        "    (cr_Ym <= SL_Y_MIN + BUFFER_M) |\n",
        "    (cr_Ym >= SL_Y_MAX - BUFFER_M)\n",
        ")\n",
        "\n",
        "pre_len = len(df)\n",
        "df = df[~edge_mask_m].copy()\n",
        "print(f\"Removed {pre_len - len(df)} crashes near dataset boundary (within ~{BUFFER_M:.0f} m of streetlight coverage edge).\")\n",
        "\n",
        "print(\"\\n=== DISTANCE DISTRIBUTION ===\")\n",
        "print(df[\"DIST_TO_LIGHT_M\"].describe(percentiles=[0.5, 0.9, 0.99]))\n",
        "\n",
        "print(\"\\nCrashes > 500 m from a light:\", (df[\"DIST_TO_LIGHT_M\"] > 500).sum())\n",
        "print(\"Crashes > 1000 m from a light:\", (df[\"DIST_TO_LIGHT_M\"] > 1000).sum())\n",
        "\n",
        "# --- Sanity check near Capitol Hill ---\n",
        "lat0, lon0 = 38.892054, -77.008611\n",
        "x0, y0 = tf_xy.transform(lon0, lat0)\n",
        "d_m, _ = tree.query(np.array([[x0, y0]]), k=1)\n",
        "print(f\"Sanity check ‚Äî nearest streetlight near Capitol Hill: {d_m[0][0]:.2f} m\")\n",
        "\n",
        "# --- Summary stats ---\n",
        "within = (df[\"DIST_TO_LIGHT_M\"] <= WITHIN_THRESHOLD_M).sum()\n",
        "total  = len(df)\n",
        "pct_within = (within / total * 100.0) if total else 0.0\n",
        "pct_far    = 100.0 - pct_within\n",
        "\n",
        "print(f\"Nighttime crashes within {WITHIN_THRESHOLD_M} m of a streetlight: {pct_within:.2f}%  ({within}/{total})\")\n",
        "print(f\"Nighttime crashes > {WITHIN_THRESHOLD_M} m from a streetlight: {pct_far:.2f}%  ({total-within}/{total})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-OJsY26V_Uz"
      },
      "source": [
        "Convert to UTM (meters instead of coordinates) and prepare the points that are outside of the distance threshold for clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUip6TTqgDR-"
      },
      "outputs": [],
      "source": [
        "# Select crashes to cluster (those > FAR_THRESHOLD_M from a light) ===\n",
        "\n",
        "print(\"\\nSelecting crashes beyond threshold distance from streetlights...\")\n",
        "\n",
        "# Step 1: Filter crashes beyond threshold\n",
        "far_df = df[df[\"DIST_TO_LIGHT_M\"] > FAR_THRESHOLD_M].copy()\n",
        "far_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "if far_df.empty:\n",
        "    print(f\"No crashes beyond {FAR_THRESHOLD_M} meters. Nothing to cluster.\")\n",
        "else:\n",
        "    print(f\"Crashes > {FAR_THRESHOLD_M} m from nearest light: {len(far_df)}\")\n",
        "\n",
        "    # Step 2: Add stable position index\n",
        "    far_df[\"ROW_POS\"] = np.arange(len(far_df))\n",
        "\n",
        "    # --- Remove duplicate locations (same lat/lon) to avoid fake dense clusters ---\n",
        "    pre_dedup = len(far_df)\n",
        "    far_df = far_df.drop_duplicates(subset=[\"LATITUDE\", \"LONGITUDE\"]).copy()\n",
        "    print(f\"Removed {pre_dedup - len(far_df)} duplicate crash points before clustering.\")\n",
        "\n",
        "    # Step 3: Project lat/lon ‚Üí UTM 18N (EPSG:32618)\n",
        "    print(\"Projecting coordinates to UTM (meters)...\")\n",
        "    from pyproj import Transformer\n",
        "    tf_xy = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32618\", always_xy=True)\n",
        "    Xm, Ym = tf_xy.transform(\n",
        "        far_df[\"LONGITUDE\"].to_numpy(),\n",
        "        far_df[\"LATITUDE\"].to_numpy()\n",
        "    )\n",
        "\n",
        "    # Step 4: Store coordinates for clustering\n",
        "    XY = np.column_stack([Xm, Ym])\n",
        "    X_arr, Y_arr = XY[:, 0], XY[:, 1]\n",
        "\n",
        "    print(f\"Projected {len(far_df)} crash points to UTM coordinates.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt-0joCIgnva"
      },
      "source": [
        "Clustering using the library complete linkage. Complete Linkage functions as so: For the actual clustering, we use a system called complete linkage. Complete linkage follow as so. It makes a clusters where the furthest distance between any two points in a cluster is not greater than the given threshold. That means that if points A, B, C and D are in a line, say 10 meters apart, and the threshold is 15 meters, the two clusters would be A,B and C,D. Not A,B,C,D. The way that it actually works is takes two points that are very close together and makes them into a cluster. It adds point after point provided that the distance between the new point and the furthest point in the cluster is less than the threshold. If it is too big, it won‚Äôt add it to that cluster and will either add it to another cluster, begin a new cluster, or it will leave it unclustered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArXaBNo5mVLa"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\nPerforming hierarchical clustering with complete linkage...\")\n",
        "\n",
        "# --- Make sure EASTING/NORTHING exist in meters ---\n",
        "# If you already have XY from a prior step:\n",
        "far_df[\"EASTING\"]  = XY[:, 0]\n",
        "far_df[\"NORTHING\"] = XY[:, 1]\n",
        "\n",
        "# If there‚Äôs any chance of NaNs, drop them to avoid fit errors\n",
        "far_xy = far_df[[\"EASTING\", \"NORTHING\"]].dropna().to_numpy()\n",
        "if far_xy.shape[0] == 0:\n",
        "    raise ValueError(\"No valid points to cluster (EASTING/NORTHING are empty after dropping NaNs).\")\n",
        "\n",
        "# --- Clustering params (meters) ---\n",
        "cluster_dist_threshold = 30  # meters\n",
        "min_points = 3\n",
        "\n",
        "clustering = AgglomerativeClustering(\n",
        "    n_clusters=None,\n",
        "    distance_threshold=cluster_dist_threshold,\n",
        "    linkage=\"complete\",\n",
        "    metric=\"euclidean\",          # explicit (default for non-ward)\n",
        "    compute_distances=False      # set True only if you plan to inspect distances_\n",
        ")\n",
        "\n",
        "# Fit clustering\n",
        "labels = clustering.fit_predict(far_xy)\n",
        "\n",
        "# Attach labels back (align lengths if we dropped NaNs)\n",
        "far_df = far_df.loc[~far_df[[\"EASTING\",\"NORTHING\"]].isna().any(axis=1)].copy()\n",
        "far_df[\"CLUSTER\"] = labels\n",
        "\n",
        "# --- Filter clusters by minimum size ---\n",
        "counts = far_df[\"CLUSTER\"].value_counts()\n",
        "valid = counts[counts >= min_points].index\n",
        "filtered_clusters = far_df[far_df[\"CLUSTER\"].isin(valid)].copy()\n",
        "\n",
        "print(f\"Found {len(counts)} clusters, {len(valid)} with at least {min_points} points\")\n",
        "print(f\"Total points after filtering: {len(filtered_clusters)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJm7b9X0oCzN"
      },
      "source": [
        "Makes the data table with following values:\n",
        "\n",
        "\n",
        "Size of the cluster (COUNT)\n",
        "\n",
        "Crash severity (SEVERITY_SUM)\n",
        "\n",
        "Distance spread metrics (MAX_R_FROM_CENTER_M, DIAMETER_M)\n",
        "\n",
        "Distance to streetlights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTRImZSHoQeb"
      },
      "outputs": [],
      "source": [
        "# === Summarize valid clusters (fixed full severity logic) ===\n",
        "print(\"\\nSummarizing valid clusters...\")\n",
        "\n",
        "fc = filtered_clusters.copy()\n",
        "\n",
        "# -------- 1. Identify all injury columns --------\n",
        "\n",
        "injury_cols = [\n",
        "    'MAJORINJURIES_BICYCLIST','MINORINJURIES_BICYCLIST','UNKNOWNINJURIES_BICYCLIST','FATAL_BICYCLIST',\n",
        "    'MAJORINJURIES_DRIVER','MINORINJURIES_DRIVER','UNKNOWNINJURIES_DRIVER','FATAL_DRIVER',\n",
        "    'MAJORINJURIES_PEDESTRIAN','MINORINJURIES_PEDESTRIAN','UNKNOWNINJURIES_PEDESTRIAN','FATAL_PEDESTRIAN',\n",
        "    'FATALPASSENGER','MAJORINJURIESPASSENGER','MINORINJURIESPASSENGER','UNKNOWNINJURIESPASSENGER',\n",
        "    'MAJORINJURIESOTHER','MINORINJURIESOTHER','UNKNOWNINJURIESOTHER','FATALOTHER'\n",
        "]\n",
        "\n",
        "injury_cols = [c for c in injury_cols if c in fc.columns]\n",
        "\n",
        "# Coerce to numeric\n",
        "for c in injury_cols:\n",
        "    fc[c] = pd.to_numeric(fc[c], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# Categorize\n",
        "fatal_cols  = [c for c in injury_cols if \"FATAL\" in c.upper()]\n",
        "major_cols  = [c for c in injury_cols if \"MAJOR\" in c.upper()]\n",
        "minor_cols  = [c for c in injury_cols if \"MINOR\" in c.upper()]\n",
        "\n",
        "fatal_any = fc[fatal_cols].sum(axis=1) > 0\n",
        "major_any = fc[major_cols].sum(axis=1) > 0\n",
        "minor_any = fc[minor_cols].sum(axis=1) > 0\n",
        "\n",
        "# Priority: fatal > major > minor\n",
        "fc[\"CRASH_SEVERITY\"] = np.select(\n",
        "    [fatal_any, major_any, minor_any],\n",
        "    [7,         4,         1],\n",
        "    default=1   # every crash counts at least 1\n",
        ")\n",
        "\n",
        "\n",
        "# -------- 2. Summarize each cluster --------\n",
        "\n",
        "summaries = []\n",
        "\n",
        "for cid in sorted(fc[\"CLUSTER\"].unique()):\n",
        "    group = fc[fc[\"CLUSTER\"] == cid]\n",
        "\n",
        "    seed_lat = group.iloc[0][\"LATITUDE\"]\n",
        "    seed_lon = group.iloc[0][\"LONGITUDE\"]\n",
        "\n",
        "    severity_sum = group[\"CRASH_SEVERITY\"].sum()\n",
        "\n",
        "    mean_dist_to_light = group[\"DIST_TO_LIGHT_M\"].mean()\n",
        "\n",
        "    dx = group[\"EASTING\"].to_numpy() - group.iloc[0][\"EASTING\"]\n",
        "    dy = group[\"NORTHING\"].to_numpy() - group.iloc[0][\"NORTHING\"]\n",
        "    dists_from_seed = np.sqrt(dx**2 + dy**2)\n",
        "    max_r = dists_from_seed.max()\n",
        "\n",
        "    coords = group[[\"EASTING\", \"NORTHING\"]].to_numpy()\n",
        "    if len(coords) <= 1:\n",
        "        diameter = 0.0\n",
        "    else:\n",
        "        diff = coords[:, None, :] - coords[None, :, :]\n",
        "        diameter = np.sqrt((diff**2).sum(axis=2)).max()\n",
        "\n",
        "    summaries.append({\n",
        "        \"CLUSTER\": cid,\n",
        "        \"COUNT\": len(group),\n",
        "        \"CENTER_LAT\": seed_lat,\n",
        "        \"CENTER_LON\": seed_lon,\n",
        "        \"SEVERITY_SUM\": severity_sum,\n",
        "        \"MEAN_DIST_TO_LIGHT_M\": mean_dist_to_light,\n",
        "        \"MAX_R_FROM_CENTER_M\": max_r,\n",
        "        \"DIAMETER_M\": diameter\n",
        "    })\n",
        "\n",
        "# -------- 3. Build simplified summary DF and print top 10 --------\n",
        "cluster_summary_df = pd.DataFrame(summaries)\n",
        "\n",
        "# Keep only the columns you want (with lon/lat) and standardize names\n",
        "cluster_simple = cluster_summary_df.rename(columns={\n",
        "    \"COUNT\": \"N_CRASHES\",\n",
        "    \"CENTER_LAT\": \"AVG_LAT\",\n",
        "    \"CENTER_LON\": \"AVG_LON\"\n",
        "})[[\"N_CRASHES\", \"SEVERITY_SUM\", \"AVG_LON\", \"AVG_LAT\"]]\n",
        "\n",
        "# Sort by severity then crashes\n",
        "cluster_simple = cluster_simple.sort_values(\n",
        "    [\"SEVERITY_SUM\", \"N_CRASHES\"], ascending=[False, False]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# Add RANK as first column\n",
        "cluster_simple.insert(0, \"RANK\", np.arange(1, len(cluster_simple) + 1))\n",
        "\n",
        "print(f\"Final clusters summarized: {len(cluster_simple)}\")\n",
        "print(\"\\nTop 10 clusters (RANK, N_CRASHES, SEVERITY_SUM, AVG_LON, AVG_LAT):\")\n",
        "display(cluster_simple.head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euTdQRX7Clnf"
      },
      "source": [
        "Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv87Z5cHCnjD"
      },
      "outputs": [],
      "source": [
        "# CELL ‚Äî Folium map: streetlights + fc crashes ONLY (raw, uniform, no outline) + TOP 10 clusters\n",
        "import folium\n",
        "from branca.colormap import linear\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\nMapping streetlights + fc crashes ONLY (raw, uniform, no outline) + TOP 10 clusters...\")\n",
        "\n",
        "# -----------------------------\n",
        "# 0) Prep fc crash points\n",
        "# -----------------------------\n",
        "fc_pts = fc[[\"LATITUDE\", \"LONGITUDE\"]].copy()\n",
        "fc_pts[\"LATITUDE\"]  = fc_pts[\"LATITUDE\"].astype(float)\n",
        "fc_pts[\"LONGITUDE\"] = fc_pts[\"LONGITUDE\"].astype(float)\n",
        "\n",
        "n_total = len(fc_pts)\n",
        "fc_pts = fc_pts.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"])\n",
        "n_valid = len(fc_pts)\n",
        "\n",
        "print(f\"fc crashes total: {n_total:,}\")\n",
        "print(f\"fc crashes with valid coords (plotted): {n_valid:,}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Map init\n",
        "# -----------------------------\n",
        "m = folium.Map(\n",
        "    location=[38.9072, -77.0369],\n",
        "    zoom_start=12,\n",
        "    tiles=\"cartodbpositron\",\n",
        "    prefer_canvas=True\n",
        ")\n",
        "m.fit_bounds([[LAT_MIN, LON_MIN], [LAT_MAX, LON_MAX]])\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Streetlights base layer\n",
        "# -----------------------------\n",
        "fg_lights = folium.FeatureGroup(name=\"Streetlights\", show=True)\n",
        "\n",
        "for _, row in streetlights.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[float(row[\"LATITUDE\"]), float(row[\"LONGITUDE\"])],\n",
        "        radius=1.2,\n",
        "        color=\"#0d6efd\",\n",
        "        fill=True,\n",
        "        fill_opacity=0.25,\n",
        "        weight=0\n",
        "    ).add_to(fg_lights)\n",
        "\n",
        "fg_lights.add_to(m)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) fc crashes ONLY (raw, uniform, NO outline)\n",
        "# -----------------------------\n",
        "fg_fc = folium.FeatureGroup(name=\"Far-from-lights crashes (fc)\", show=True)\n",
        "\n",
        "for lat, lon in fc_pts[[\"LATITUDE\", \"LONGITUDE\"]].to_numpy():\n",
        "    folium.CircleMarker(\n",
        "        location=[lat, lon],\n",
        "        radius=3.0,\n",
        "        stroke=False,          # NO outline\n",
        "        fill=True,\n",
        "        fill_color=\"#3388ff\",  # uniform color\n",
        "        fill_opacity=0.6\n",
        "    ).add_to(fg_fc)\n",
        "\n",
        "fg_fc.add_to(m)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Top 10 clusters (severity-scaled)\n",
        "# -----------------------------\n",
        "top_clusters = (\n",
        "    cluster_summary_df\n",
        "    .sort_values([\"SEVERITY_SUM\", \"COUNT\"], ascending=[False, False])\n",
        "    .head(10)\n",
        "    .copy()\n",
        ")\n",
        "\n",
        "if not top_clusters.empty:\n",
        "    vmin_cl = float(top_clusters[\"SEVERITY_SUM\"].min())\n",
        "    vmax_cl = float(top_clusters[\"SEVERITY_SUM\"].max())\n",
        "    if vmin_cl == vmax_cl:\n",
        "        vmin_cl = 0.0\n",
        "\n",
        "    cmap_cl = linear.Reds_09.scale(vmin_cl, vmax_cl)\n",
        "    fg_cl = folium.FeatureGroup(name=\"Top 10 clusters (by severity sum)\", show=True)\n",
        "\n",
        "    for _, row in top_clusters.iterrows():\n",
        "        sev_sum = float(row[\"SEVERITY_SUM\"])\n",
        "        n_crashes = int(row[\"COUNT\"])\n",
        "\n",
        "        radius = (\n",
        "            15.0 if vmax_cl == vmin_cl\n",
        "            else 10.0 + 25.0 * (sev_sum - vmin_cl) / (vmax_cl - vmin_cl)\n",
        "        )\n",
        "\n",
        "        popup = (\n",
        "            f\"<b>Cluster ID:</b> {int(row['CLUSTER'])}<br>\"\n",
        "            f\"<b>Crashes:</b> {n_crashes}<br>\"\n",
        "            f\"<b>Severity sum:</b> {int(sev_sum)}<br>\"\n",
        "            f\"<b>Mean dist to light (m):</b> {float(row['MEAN_DIST_TO_LIGHT_M']):.1f}<br>\"\n",
        "            f\"<b>Diameter (m):</b> {float(row['DIAMETER_M']):.1f}\"\n",
        "        )\n",
        "\n",
        "        color = cmap_cl(sev_sum)\n",
        "\n",
        "        folium.CircleMarker(\n",
        "            location=[float(row[\"CENTER_LAT\"]), float(row[\"CENTER_LON\"])],\n",
        "            radius=radius,\n",
        "            color=color,\n",
        "            fill=True,\n",
        "            fill_color=color,\n",
        "            fill_opacity=0.9,\n",
        "            popup=folium.Popup(popup, max_width=360),\n",
        "            tooltip=f\"Cluster {int(row['CLUSTER'])} | SevSum {int(sev_sum)}\"\n",
        "        ).add_to(fg_cl)\n",
        "\n",
        "    fg_cl.add_to(m)\n",
        "    cmap_cl.caption = \"Cluster severity (SEVERITY_SUM)\"\n",
        "    cmap_cl.add_to(m)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Layer control\n",
        "# -----------------------------\n",
        "folium.LayerControl(collapsed=False).add_to(m)\n",
        "\n",
        "print(\"‚úÖ Map ready: ONLY fc crashes (raw, uniform, no outline) + top 10 clusters + streetlights.\")\n",
        "m\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}