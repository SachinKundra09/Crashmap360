{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installations"
      ],
      "metadata": {
        "id": "GPEkUToWDrzY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeLI9sHnDk4D"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paths & Parameters"
      ],
      "metadata": {
        "id": "XFiKem-tDu4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# Paths (EDIT IF NEEDED)\n",
        "# -------------------\n",
        "CRASHES_PATH       = \"/content/drive/My Drive/Crashes_in_DC.csv\"\n",
        "CAMERAS_PATH       = \"/content/drive/My Drive/Automated_Traffic_Enforcement.csv\"\n",
        "SPEEDHUMPS_PATH    = \"/content/drive/My Drive/Speed_Humps.csv\"\n",
        "OUT_DIR            = \"/content/drive/My Drive/outputs\"\n",
        "SPEED_GEOJSON_PATH = \"/content/drive/My Drive/Roadway_SubBlock.geojson\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------\n",
        "# Parameters\n",
        "# -------------------\n",
        "DATE_START = \"2020-01-01\"\n",
        "DATE_END   = \"2025-04-30\"\n",
        "MAR_MIN    = 100\n"
      ],
      "metadata": {
        "id": "MwZrWFd6D6Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions, gets rid of junk data and gives everything a standardized name."
      ],
      "metadata": {
        "id": "CJ78gkV3EFgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_intersection(s):\n",
        "    if s is None or (isinstance(s, float) and np.isnan(s)): return \"\"\n",
        "    s = str(s).strip()\n",
        "    if re.fullmatch(r\"(nan|none|null|missing)\", s, flags=re.I): return \"\"\n",
        "    if \"Intersecting RouteID\" in s or \"*\" in s: return \"\"\n",
        "    s = re.sub(r\"\\s*[/@&]\\s*\", \" & \", s)\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "def extract_street_name(row):\n",
        "    txt = None\n",
        "    if \"MAR_Address\" in row and isinstance(row[\"MAR_Address\"], str):\n",
        "        txt = row[\"MAR_Address\"]\n",
        "    elif \"ADDRESS\" in row and isinstance(row[\"ADDRESS\"], str):\n",
        "        txt = row[\"ADDRESS\"]\n",
        "    if txt:\n",
        "        t = txt.upper().strip()\n",
        "        t = re.split(r\"\\s*&\\s*|\\s*@\\s*|\\s*/\\s*|,| - \", t)[0]\n",
        "        t = re.sub(r\"\\b(NE|NW|SE|SW)\\b\", \"\", t)\n",
        "        return re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    inter = str(row.get(\"NEARESTINTSTREETNAME\", \"\")).upper().strip()\n",
        "    if inter:\n",
        "        return re.split(r\"\\s*&\\s*|\\s*@\\s*|\\s*/\\s*|,| - \", inter)[0].strip()\n",
        "    return \"\"\n",
        "\n",
        "def standardize_name(df, out_col=\"NAME\", candidates=None, fallback_series=None):\n",
        "    if candidates is None: candidates = []\n",
        "    out = pd.Series(\"\", index=df.index, dtype=object)\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            cand = df[c].fillna(\"\").astype(str).str.strip()\n",
        "            out = np.where(out != \"\", out, cand)\n",
        "    if fallback_series is not None:\n",
        "        fb = fallback_series.fillna(\"\").astype(str).str.strip()\n",
        "        out = np.where(out != \"\", out, fb)\n",
        "    out = pd.Series(out, index=df.index).replace(\"\", \"(UNNAMED)\")\n",
        "    df[out_col] = out\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "9rRsaEvKEMoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and clean crashes. Filter for MAR score and for date.\n",
        "Also define the calculations for the severity sum."
      ],
      "metadata": {
        "id": "hCfppzGVEX7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# 4) Load & clean crashes (WITH severity score; no distances)\n",
        "# -------------------\n",
        "df = pd.read_csv(CRASHES_PATH, dtype={\"STREETSEGID\": str}, low_memory=False)\n",
        "\n",
        "# Keep speeding-related only\n",
        "df[\"SPEEDING_INVOLVED\"] = pd.to_numeric(df[\"SPEEDING_INVOLVED\"], errors=\"coerce\")\n",
        "df = df[df[\"SPEEDING_INVOLVED\"] > 0].copy()\n",
        "\n",
        "# Valid lat/lon\n",
        "df = df.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"]).copy()\n",
        "df[\"LATITUDE\"]  = pd.to_numeric(df[\"LATITUDE\"], errors=\"coerce\")\n",
        "df[\"LONGITUDE\"] = pd.to_numeric(df[\"LONGITUDE\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"]).copy()\n",
        "\n",
        "# Date window\n",
        "df[\"FROMDATE\"] = pd.to_datetime(df[\"FROMDATE\"], errors=\"coerce\")\n",
        "df = df[(df[\"FROMDATE\"] >= DATE_START) & (df[\"FROMDATE\"] <= DATE_END)].copy()\n",
        "\n",
        "# MAR quality (if present)\n",
        "if \"MAR_SCORE\" in df.columns:\n",
        "    df[\"MAR_SCORE\"] = pd.to_numeric(df[\"MAR_SCORE\"], errors=\"coerce\")\n",
        "    df = df[df[\"MAR_SCORE\"] >= MAR_MIN].copy()\n",
        "\n",
        "# ----- Injury columns & SEVERITY_SCORE -----\n",
        "# Define injury categories (robust to missing columns)\n",
        "injury_categories = {\n",
        "    \"BICYCLIST\":  [\"MAJORINJURIES_BICYCLIST\",\"MINORINJURIES_BICYCLIST\",\"UNKNOWNINJURIES_BICYCLIST\",\"FATAL_BICYCLIST\"],\n",
        "    \"DRIVER\":     [\"MAJORINJURIES_DRIVER\",\"MINORINJURIES_DRIVER\",\"UNKNOWNINJURIES_DRIVER\",\"FATAL_DRIVER\"],\n",
        "    \"PEDESTRIAN\": [\"MAJORINJURIES_PEDESTRIAN\",\"MINORINJURIES_PEDESTRIAN\",\"UNKNOWNINJURIES_PEDESTRIAN\",\"FATAL_PEDESTRIAN\"],\n",
        "    \"PASSENGER\":  [\"MAJORINJURIESPASSENGER\",\"MINORINJURIESPASSENGER\",\"FATALPASSENGER\"],\n",
        "    \"OTHER\":      [\"MAJORINJURIESOTHER\",\"MINORINJURIESOTHER\",\"FATALOTHER\"],\n",
        "}\n",
        "\n",
        "fatal_cols = [c for cols in injury_categories.values() for c in cols if \"FATAL\" in c]\n",
        "major_cols = [c for cols in injury_categories.values() for c in cols if \"MAJOR\" in c]\n",
        "minor_cols = [c for cols in injury_categories.values() for c in cols if \"MINOR\" in c]\n",
        "\n",
        "# Ensure all referenced injury columns exist & are numeric\n",
        "for col in set(fatal_cols + major_cols + minor_cols):\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
        "    else:\n",
        "        df[col] = 0\n",
        "\n",
        "# Keep only crashes with at least one injury (exclude property-only)\n",
        "injury_cols_all = fatal_cols + major_cols + minor_cols\n",
        "df = df[df[injury_cols_all].sum(axis=1) > 0].copy()\n",
        "\n",
        "# Weighted severity score\n",
        "df[\"SEVERITY_SCORE\"] = (\n",
        "    7 * df[fatal_cols].sum(axis=1) +\n",
        "    4 * df[major_cols].sum(axis=1) +\n",
        "    1 * df[minor_cols].sum(axis=1)\n",
        ")\n",
        "\n",
        "# ----- Standardize names (no geometry ops yet) -----\n",
        "if \"NEARESTINTSTREETNAME\" not in df.columns:\n",
        "    df[\"NEARESTINTSTREETNAME\"] = \"\"\n",
        "df[\"NEARESTINTSTREETNAME\"] = df[\"NEARESTINTSTREETNAME\"].apply(clean_intersection)\n",
        "df[\"PRIMARY_STREET\"] = df.apply(extract_street_name, axis=1).str.upper()\n",
        "\n",
        "df = standardize_name(\n",
        "    df,\n",
        "    out_col=\"NAME\",\n",
        "    candidates=[\"PRIMARY_STREET\",\"NEARESTINTSTREETNAME\"],\n",
        "    fallback_series=None\n",
        ")\n",
        "\n",
        "# (No geometry or projections here; distances come later.)\n",
        "print(f\"[Crashes after filters] {len(df):,} rows, with SEVERITY_SCORE computed.\")"
      ],
      "metadata": {
        "id": "tw0L78VtEU65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load cameras and humps"
      ],
      "metadata": {
        "id": "OltzdY-EEc9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cams = pd.read_csv(CAMERAS_PATH)\n",
        "cams = cams.rename(columns={\"CAMERA_LATITUDE\": \"LATITUDE\",\"CAMERA_LONGITUDE\": \"LONGITUDE\"})\n",
        "cams = cams.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"]).copy()\n",
        "cams[\"LATITUDE\"]  = pd.to_numeric(cams[\"LATITUDE\"], errors=\"coerce\")\n",
        "cams[\"LONGITUDE\"] = pd.to_numeric(cams[\"LONGITUDE\"], errors=\"coerce\")\n",
        "cams = cams.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"]).copy()\n",
        "\n",
        "# Speed humps\n",
        "humps = pd.read_csv(SPEEDHUMPS_PATH)\n",
        "humps = humps.dropna(subset=[\"LATITUDE\",\"LONGITUDE\"]).copy()\n",
        "humps[\"LATITUDE\"]  = pd.to_numeric(humps[\"LATITUDE\"], errors=\"coerce\")\n",
        "humps[\"LONGITUDE\"] = pd.to_numeric(humps[\"LONGITUDE\"], errors=\"coerce\")\n",
        "humps = humps.dropna(subset=[\"LATITUDE\",\"LONGITUDE\"]).copy()"
      ],
      "metadata": {
        "id": "UhlnAYILEgW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summaries of total number accidents, speed cameras, and speedbumps to help figure out what time of clustering should be done"
      ],
      "metadata": {
        "id": "vpr0NE8cElRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Current Totals (pre-distance) ===\")\n",
        "print(f\"Crashes being counted: {len(df):,}\")\n",
        "print(f\"Speed cameras: {len(cams):,}\")\n",
        "print(f\"Speed humps:   {len(humps):,}\")\n",
        "print(f\"Total devices: {len(cams) + len(humps):,}\")"
      ],
      "metadata": {
        "id": "4mmucYNzEvGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are few crashes and total devices, so we can just do brute force distance calculations using haversine metrics to figure out distance between each crash and its nearest device. We can do this because there are only a total of 1,210 * 589 different calculations that are being run. I also used Euclidean distances as I did in the night accidents"
      ],
      "metadata": {
        "id": "ZLbweLkDEuUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we do that though, we must assign all speed bumps, cameras, and accidents onto their actual street so that when we do our distance calculations they are all on the proper road"
      ],
      "metadata": {
        "id": "Miptmoi0WzR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the DDOT lines"
      ],
      "metadata": {
        "id": "P0ROHDptXNVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.a) DDOT lines → standardized street name (\"street_base\") ---\n",
        "import re, numpy as np, pandas as pd, geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "CRS_METERS = 32618\n",
        "MAX_LATERAL_M = 20  # lateral tolerance to snap a point to a street\n",
        "\n",
        "# Load and project\n",
        "gdf_limits_4326 = gpd.read_file(SPEED_GEOJSON_PATH)\n",
        "if gdf_limits_4326.crs is None:\n",
        "    gdf_limits_4326 = gdf_limits_4326.set_crs(4326)\n",
        "gdf_limits_m = gdf_limits_4326.to_crs(CRS_METERS).copy()\n",
        "\n",
        "# Pick a DDOT street-name column and normalize it\n",
        "name_cands_ddot = [c for c in [\"STREETNAME\",\"ROUTENAME\",\"NAME\",\"FULLNAME\",\"SEGMENTNAME\"] if c in gdf_limits_m.columns]\n",
        "ddot_name_col = name_cands_ddot[0] if name_cands_ddot else None\n",
        "\n",
        "def norm_street(s: str) -> str:\n",
        "    if s is None or (isinstance(s,float) and np.isnan(s)): return \"\"\n",
        "    s = str(s).upper().strip()\n",
        "    # remove quadrant suffixes and stray punctuation\n",
        "    s = re.sub(r\"\\b(NE|NW|SE|SW)\\b\", \"\", s)\n",
        "    s = re.sub(r\"[^A-Z0-9 &'-]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "if ddot_name_col:\n",
        "    gdf_limits_m[\"street_base\"] = gdf_limits_m[ddot_name_col].apply(norm_street)\n",
        "else:\n",
        "    # no name column? fallback: empty (we can still use nearest line later if needed)\n",
        "    gdf_limits_m[\"street_base\"] = \"\"\n",
        "\n",
        "# Keep geometry + street_base only (index will serve as a line id if needed later)\n",
        "ddot_lines = gdf_limits_m[[\"geometry\",\"street_base\"]].copy()\n"
      ],
      "metadata": {
        "id": "IzsrXxiEXFAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign the devices to the nearest DDOT line"
      ],
      "metadata": {
        "id": "QLjpA4fhaPh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) ASSIGN DEVICES — map to nearest DDOT line, inherit street_base\n",
        "# Expects: cams, humps dataframes with LONGITUDE/LATITUDE\n",
        "\n",
        "# Build device GeoDataFrames (WGS84 → meters)\n",
        "gdf_cams_4326  = gpd.GeoDataFrame(cams.copy(),  geometry=gpd.points_from_xy(cams[\"LONGITUDE\"],  cams[\"LATITUDE\"]),  crs=4326)\n",
        "gdf_humps_4326 = gpd.GeoDataFrame(humps.copy(), geometry=gpd.points_from_xy(humps[\"LONGITUDE\"], humps[\"LATITUDE\"]), crs=4326)\n",
        "gdf_cams_4326[\"device_kind\"]  = \"camera\"\n",
        "gdf_humps_4326[\"device_kind\"] = \"hump\"\n",
        "\n",
        "cams_m    = gdf_cams_4326.to_crs(CRS_METERS)\n",
        "humps_m   = gdf_humps_4326.to_crs(CRS_METERS)\n",
        "devices_m = pd.concat([cams_m[[\"geometry\",\"device_kind\"]],\n",
        "                       humps_m[[\"geometry\",\"device_kind\"]]], ignore_index=True)\n",
        "\n",
        "# Snap to nearest DDOT line; inherit street_base\n",
        "dev2line = gpd.sjoin_nearest(devices_m, ddot_lines, how=\"left\", distance_col=\"dev_dist_to_line_m\") \\\n",
        "             .rename(columns={\"index_right\":\"ddot_idx\"}) \\\n",
        "             .reset_index().rename(columns={\"index\":\"device_index\"})\n",
        "\n",
        "dev2line[\"dev_valid_line\"] = dev2line[\"dev_dist_to_line_m\"] <= MAX_LATERAL_M\n",
        "dev2line[\"street_base\"]    = dev2line[\"street_base\"].fillna(\"\").astype(str)\n",
        "\n",
        "print(\"Devices valid to street:\", int(dev2line[\"dev_valid_line\"].sum()), \"/\", len(dev2line))\n"
      ],
      "metadata": {
        "id": "AtwWfi8oaY6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign crashes to their nearest DDOT line"
      ],
      "metadata": {
        "id": "t6noictVahLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) ASSIGN CRASHES — map to nearest DDOT line, inherit street_base\n",
        "# Expects: df (filtered crashes) with LONGITUDE/LATITUDE\n",
        "\n",
        "gdf_crashes_4326 = gpd.GeoDataFrame(df.copy(), geometry=gpd.points_from_xy(df[\"LONGITUDE\"], df[\"LATITUDE\"]), crs=4326)\n",
        "crashes_m = gdf_crashes_4326.to_crs(CRS_METERS)\n",
        "\n",
        "cr2line = gpd.sjoin_nearest(crashes_m, ddot_lines, how=\"left\", distance_col=\"cr_dist_to_line_m\") \\\n",
        "           .rename(columns={\"index_right\":\"ddot_idx\"}) \\\n",
        "           .reset_index().rename(columns={\"index\":\"crash_index\"})\n",
        "\n",
        "cr2line[\"cr_valid_line\"] = cr2line[\"cr_dist_to_line_m\"] <= MAX_LATERAL_M\n",
        "cr2line[\"street_base\"]   = cr2line[\"street_base\"].fillna(\"\").astype(str)\n",
        "\n",
        "print(\"Crashes valid to street:\", int(cr2line[\"cr_valid_line\"].sum()), \"/\", len(cr2line))\n"
      ],
      "metadata": {
        "id": "vpws_Blhajcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop any device or crashes not assigned to ddot line"
      ],
      "metadata": {
        "id": "IKOpzfW4a44t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.5) DROP INVALID — keep only records with a valid street assignment\n",
        "before_dev, before_cr = len(dev2line), len(cr2line)\n",
        "\n",
        "dev_on_street = dev2line[dev2line[\"dev_valid_line\"] & dev2line[\"street_base\"].ne(\"\")].copy()\n",
        "cr_on_street  = cr2line[cr2line[\"cr_valid_line\"] & cr2line[\"street_base\"].ne(\"\")].copy()\n",
        "\n",
        "print(\"Devices kept:\", len(dev_on_street), \"/\", before_dev,\n",
        "      \"| Crashes kept:\", len(cr_on_street), \"/\", before_cr)\n"
      ],
      "metadata": {
        "id": "GDPHCDCta7id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now caculate the euclidean distances"
      ],
      "metadata": {
        "id": "mgG-IaK8a-FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) DISTANCES — nearest device on the SAME STREET (keep INF if none)\n",
        "\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "\n",
        "# Start from devices_m (has geometry + device_kind) and add street_base from dev_on_street\n",
        "devices_m_idxed = devices_m.reset_index().rename(columns={\"index\": \"device_index\"})  # geometry, device_kind\n",
        "devices_m_idxed = devices_m_idxed.merge(\n",
        "    dev_on_street[[\"device_index\", \"street_base\"]],\n",
        "    on=\"device_index\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "rows = []\n",
        "for street, cr_grp in cr_on_street.groupby(\"street_base\"):\n",
        "    dev_grp = devices_m_idxed[devices_m_idxed[\"street_base\"] == street]\n",
        "\n",
        "    if dev_grp.empty:\n",
        "        tmp = cr_grp.copy()\n",
        "        tmp[\"dist_to_device_same_street_m\"] = np.inf\n",
        "        tmp[\"nearest_device_index\"] = pd.NA\n",
        "        tmp[\"nearest_device_kind\"]  = pd.NA\n",
        "        rows.append(tmp)\n",
        "        continue\n",
        "\n",
        "    dev_gdf = gpd.GeoDataFrame(\n",
        "        dev_grp[[\"geometry\", \"device_index\", \"device_kind\"]].copy(),\n",
        "        geometry=\"geometry\",\n",
        "        crs=CRS_METERS\n",
        "    )\n",
        "\n",
        "    tmp = gpd.sjoin_nearest(\n",
        "        cr_grp.set_geometry(\"geometry\"),\n",
        "        dev_gdf,\n",
        "        how=\"left\",\n",
        "        distance_col=\"dist_to_device_same_street_m\"\n",
        "    ).rename(columns={\"index_right\": \"_dev_join_row\"}).copy()\n",
        "\n",
        "    tmp[\"nearest_device_index\"] = tmp[\"device_index\"]\n",
        "    tmp[\"nearest_device_kind\"]  = tmp[\"device_kind\"]\n",
        "    rows.append(tmp)\n",
        "\n",
        "same_street = pd.concat(rows, ignore_index=True) if rows else cr_on_street.copy()\n",
        "\n",
        "finite = np.isfinite(same_street[\"dist_to_device_same_street_m\"])\n",
        "print(\"Same-street nearest-device distances computed.\")\n",
        "print(f\"Finite distances: {int(finite.sum())} of {len(same_street)}\")\n",
        "print(\n",
        "    same_street[[\"crash_index\", \"street_base\", \"dist_to_device_same_street_m\"]]\n",
        "    .head(8)\n",
        "    .to_string(index=False)\n",
        ")\n"
      ],
      "metadata": {
        "id": "3yFV8h1Bb-0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "compute the avg block size from ddot"
      ],
      "metadata": {
        "id": "FHpZ_92lgyC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) THRESHOLD — compute from DDOT segment lengths (block size)\n",
        "\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "\n",
        "# Ensure DDOT lines exist and are in meters CRS\n",
        "try:\n",
        "    _ = gdf_limits_m\n",
        "except NameError:\n",
        "    gdf_limits_4326 = gpd.read_file(SPEED_GEOJSON_PATH)\n",
        "    if gdf_limits_4326.crs is None:\n",
        "        gdf_limits_4326 = gdf_limits_4326.set_crs(4326)\n",
        "    gdf_limits_m = gdf_limits_4326.to_crs(CRS_METERS).copy()\n",
        "\n",
        "# Compute each DDOT segment's length (meters)\n",
        "gdf_limits_m[\"seg_len_m\"] = gdf_limits_m.geometry.length\n",
        "\n",
        "# Mean and median block sizes\n",
        "mean_len   = float(gdf_limits_m[\"seg_len_m\"].mean())\n",
        "median_len = float(gdf_limits_m[\"seg_len_m\"].median())\n",
        "\n",
        "# ✅ Use the MEDIAN block length as the threshold\n",
        "THRESHOLD_M = median_len\n",
        "\n",
        "print(f\"Block length — mean: {mean_len:,.1f} m | median: {median_len:,.1f} m\")\n",
        "print(f\"Using THRESHOLD_M = {THRESHOLD_M:,.1f} m (median block size)\")\n"
      ],
      "metadata": {
        "id": "No9kpngshU7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove the crashes that are inside of the threshold so that clustering can be ready"
      ],
      "metadata": {
        "id": "VLDWuk1Sh6Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) FILTER — label within/outside by THRESHOLD_M, but KEEP outside/inf for clustering\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Expect same_street from Cell 4\n",
        "is_finite   = np.isfinite(same_street[\"dist_to_device_same_street_m\"])\n",
        "within_mask = is_finite & (same_street[\"dist_to_device_same_street_m\"] <= THRESHOLD_M)\n",
        "\n",
        "kept_within_threshold     = same_street[within_mask].copy()\n",
        "remaining_for_clustering  = same_street[~within_mask].copy()   # includes finite>THRESHOLD_M and inf\n",
        "\n",
        "within_pct  = (len(kept_within_threshold) / len(same_street)) * 100\n",
        "outside_pct = 100 - within_pct\n",
        "\n",
        "print(f\"Within threshold: {len(kept_within_threshold):,} / {len(same_street):,} \"\n",
        "      f\"({within_pct:.2f}%)\")\n",
        "print(f\"Outside threshold: {len(remaining_for_clustering):,} / {len(same_street):,} \"\n",
        "      f\"({outside_pct:.2f}%)\")\n",
        "\n",
        "finite_outside = remaining_for_clustering[np.isfinite(remaining_for_clustering[\"dist_to_device_same_street_m\"])]\n",
        "infinite_rows  = remaining_for_clustering[~np.isfinite(remaining_for_clustering[\"dist_to_device_same_street_m\"])]\n",
        "\n",
        "print(f\"Remaining for clustering: {len(remaining_for_clustering):,} \"\n",
        "      f\"(finite>{THRESHOLD_M:.1f} m: {len(finite_outside):,}; inf: {len(infinite_rows):,})\")\n",
        "\n",
        "# Minimal inputs for clustering (needs the DDOT segment id)\n",
        "cluster_input = remaining_for_clustering[[\"crash_index\",\"ddot_idx\",\"geometry\"]].copy()\n",
        "cluster_input = cluster_input.rename(columns={\"ddot_idx\":\"cr_ddot_idx\"})\n"
      ],
      "metadata": {
        "id": "zO4HOFuIiC0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering using complete linkage with the 59.4 meter threshold (the median size of a block in dc)"
      ],
      "metadata": {
        "id": "GRjSyylGkYn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) CLUSTER — complete linkage per DDOT segment using THRESHOLD_M as cutoff\n",
        "\n",
        "import numpy as np\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "\n",
        "# Use the same median-based threshold from Cell 5\n",
        "CLUSTER_CUTOFF_M = THRESHOLD_M\n",
        "\n",
        "# ✅ Reset index so positions are 0..N-1 (prevents IndexError)\n",
        "cluster_input = cluster_input.reset_index(drop=True)\n",
        "\n",
        "labels = np.full(len(cluster_input), -1, dtype=int)\n",
        "\n",
        "for seg_id, grp in cluster_input.groupby(\"cr_ddot_idx\"):\n",
        "    idx = grp.index.to_numpy()  # now 0..N-1 safe\n",
        "    coords = np.c_[grp.geometry.x.values, grp.geometry.y.values]\n",
        "\n",
        "    n = len(coords)\n",
        "    if n == 1:\n",
        "        labels[idx] = 0\n",
        "        continue\n",
        "    if n == 2:\n",
        "        d = np.linalg.norm(coords[0] - coords[1])\n",
        "        if d <= CLUSTER_CUTOFF_M:\n",
        "            labels[idx] = 0\n",
        "        else:\n",
        "            labels[idx[0]] = 0\n",
        "            labels[idx[1]] = 1\n",
        "        continue\n",
        "\n",
        "    # Hierarchical (complete-linkage) clustering in Euclidean meters\n",
        "    Z = linkage(coords, method=\"complete\", metric=\"euclidean\")\n",
        "    lbl = fcluster(Z, t=CLUSTER_CUTOFF_M, criterion=\"distance\") - 1  # zero-based\n",
        "    labels[idx] = lbl\n",
        "\n",
        "cluster_input[\"cluster_id_complete\"] = labels\n",
        "\n",
        "print(f\"Complete-linkage clustering done. Cutoff = {CLUSTER_CUTOFF_M:.1f} m (median block size)\")\n",
        "print(cluster_input[[\"cr_ddot_idx\",\"cluster_id_complete\"]].head(10).to_string(index=False))\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQZcD7PkXvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Table"
      ],
      "metadata": {
        "id": "qG30qKRimUpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "#  Cluster Severity Table (RANK, N_CRASHES, SEVERITY_SUM, AVG_LON, AVG_LAT)\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from IPython.display import display\n",
        "\n",
        "# 1) Ensure one row per crash in cluster_input\n",
        "clust_unique = cluster_input.drop_duplicates(\"crash_index\").copy()\n",
        "\n",
        "# 2) Identify injury columns from same_street\n",
        "injury_cols = [c for c in same_street.columns if isinstance(c, str) and (\n",
        "    \"FATAL\" in c.upper() or \"MAJOR\" in c.upper() or \"MINOR\" in c.upper()\n",
        ")]\n",
        "for c in injury_cols:\n",
        "    same_street[c] = pd.to_numeric(same_street[c], errors=\"coerce\").fillna(0)\n",
        "\n",
        "fatal_cols = [c for c in injury_cols if \"FATAL\" in c.upper()]\n",
        "major_cols = [c for c in injury_cols if \"MAJOR\" in c.upper()]\n",
        "minor_cols = [c for c in injury_cols if \"MINOR\" in c.upper()]\n",
        "\n",
        "# 3) Collapse to one row per crash and compute PRIORITY severity\n",
        "ss1 = (\n",
        "    same_street[[\"crash_index\"] + fatal_cols + major_cols + minor_cols]\n",
        "    .groupby(\"crash_index\", as_index=False)\n",
        "    .max()\n",
        ")\n",
        "\n",
        "has_fatal = (ss1[fatal_cols].sum(axis=1) > 0) if fatal_cols else False\n",
        "has_major = (ss1[major_cols].sum(axis=1) > 0) if major_cols else False\n",
        "has_minor = (ss1[minor_cols].sum(axis=1) > 0) if minor_cols else False\n",
        "\n",
        "sev_priority = np.select(\n",
        "    [has_fatal, has_major, has_minor],\n",
        "    [7, 4, 1],\n",
        "    default=0\n",
        ").astype(float)\n",
        "\n",
        "sev_per_crash = ss1[[\"crash_index\"]].copy()\n",
        "sev_per_crash[\"SEVERITY_SCORE\"] = sev_priority\n",
        "\n",
        "# 4) Join priority severity to the unique cluster points\n",
        "ci = clust_unique.merge(sev_per_crash, on=\"crash_index\", how=\"left\")\n",
        "ci[\"SEVERITY_SCORE\"] = pd.to_numeric(ci[\"SEVERITY_SCORE\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# 5) Aggregate by cluster and compute centroids in meters\n",
        "ci[\"x\"] = ci.geometry.x\n",
        "ci[\"y\"] = ci.geometry.y\n",
        "\n",
        "cluster_stats = (\n",
        "    ci.groupby([\"cr_ddot_idx\", \"cluster_id_complete\"], as_index=False)\n",
        "      .agg(\n",
        "          n_crashes=(\"crash_index\", \"size\"),\n",
        "          severity_sum=(\"SEVERITY_SCORE\", \"sum\"),\n",
        "          avg_x=(\"x\", \"mean\"),\n",
        "          avg_y=(\"y\", \"mean\")\n",
        "      )\n",
        ")\n",
        "\n",
        "# 6) Convert centroids to lon/lat for mapping\n",
        "centers_g = gpd.GeoDataFrame(\n",
        "    cluster_stats,\n",
        "    geometry=gpd.points_from_xy(cluster_stats[\"avg_x\"], cluster_stats[\"avg_y\"]),\n",
        "    crs=CRS_METERS\n",
        ").to_crs(4326)\n",
        "\n",
        "cluster_df = (\n",
        "    centers_g\n",
        "    .assign(\n",
        "        avg_lon=lambda d: d.geometry.x,\n",
        "        avg_lat=lambda d: d.geometry.y\n",
        "    )\n",
        "    .drop(columns=[\"geometry\", \"cr_ddot_idx\", \"cluster_id_complete\", \"avg_x\", \"avg_y\"])\n",
        ")\n",
        "\n",
        "# 7) Keep only the requested columns, make ALL CAPS, add RANK\n",
        "cluster_df = cluster_df[[\"n_crashes\", \"severity_sum\", \"avg_lon\", \"avg_lat\"]]\n",
        "cluster_df = cluster_df.rename(columns=str.upper)\n",
        "\n",
        "# Sort by severity then crashes\n",
        "cluster_df = cluster_df.sort_values(\n",
        "    [\"SEVERITY_SUM\", \"N_CRASHES\"], ascending=[False, False]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# Add rank column at front\n",
        "cluster_df.insert(0, \"RANK\", np.arange(1, len(cluster_df) + 1))\n",
        "\n",
        "print(\"Cluster Table (RANK, N_CRASHES, SEVERITY_SUM, AVG_LON, AVG_LAT):\")\n",
        "display(cluster_df.head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "Q-aPi4IcmWcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "map"
      ],
      "metadata": {
        "id": "BMFfaWkLo6Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === MAP: All speeding crashes (uniform style) + all devices + TOP 10 clusters (size+color by SEVERITY_SUM) ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "from branca.colormap import linear\n",
        "\n",
        "# 0) Ensure devices exist in WGS84\n",
        "try:\n",
        "    _ = gdf_cams_4326\n",
        "    _ = gdf_humps_4326\n",
        "except NameError:\n",
        "    gdf_cams_4326  = gpd.GeoDataFrame(\n",
        "        cams.copy(),\n",
        "        geometry=gpd.points_from_xy(cams[\"LONGITUDE\"], cams[\"LATITUDE\"]),\n",
        "        crs=4326\n",
        "    )\n",
        "    gdf_humps_4326 = gpd.GeoDataFrame(\n",
        "        humps.copy(),\n",
        "        geometry=gpd.points_from_xy(humps[\"LONGITUDE\"], humps[\"LATITUDE\"]),\n",
        "        crs=4326\n",
        "    )\n",
        "\n",
        "# 1) Convert crashes to WGS84\n",
        "crashes_4326 = gpd.GeoDataFrame(same_street.copy(), geometry=\"geometry\", crs=CRS_METERS).to_crs(4326)\n",
        "\n",
        "# 2) Top 10 clusters by SEVERITY_SUM (highest)\n",
        "if \"cluster_df\" in globals() and isinstance(cluster_df, pd.DataFrame) and not cluster_df.empty:\n",
        "    top10_clusters = cluster_df.sort_values(\"SEVERITY_SUM\", ascending=False).head(10).copy()\n",
        "else:\n",
        "    top10_clusters = pd.DataFrame()\n",
        "\n",
        "# 3) Cluster styling (variable color + radius by SEVERITY_SUM like your sample)\n",
        "if not top10_clusters.empty:\n",
        "    vmin = float(top10_clusters[\"SEVERITY_SUM\"].min())\n",
        "    vmax = float(top10_clusters[\"SEVERITY_SUM\"].max())\n",
        "    if vmin == vmax:\n",
        "        vmin = 0.0\n",
        "    cmap = linear.Reds_09.scale(vmin, vmax)\n",
        "\n",
        "    def radius_by_sev(sev):\n",
        "        sev = float(sev)\n",
        "        return 6 if vmax == vmin else 6 + 20 * (sev - vmin) / (vmax - vmin)\n",
        "else:\n",
        "    cmap = None\n",
        "    def radius_by_sev(sev):  # unused\n",
        "        return 10\n",
        "\n",
        "# 4) Build the map\n",
        "m = folium.Map(location=[38.9072, -77.0369], zoom_start=12, tiles=None, max_zoom=19)\n",
        "folium.TileLayer('cartodbpositron', name='OSM (Carto Positron)', control=True).add_to(m)\n",
        "folium.TileLayer(\n",
        "    tiles=\"https://services.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}\",\n",
        "    attr=\"Esri, Maxar, Earthstar Geographics, and the GIS User Community\",\n",
        "    name=\"ESRI World Street (z≤19)\", max_zoom=19, control=True\n",
        ").add_to(m)\n",
        "\n",
        "# 5) Layer: All devices (cameras & humps)\n",
        "fg_cams  = folium.FeatureGroup(name=\"Devices – Speed Cameras\", show=False)\n",
        "for _, r in gdf_cams_4326.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[float(r.geometry.y), float(r.geometry.x)],\n",
        "        radius=3, color=\"#1976d2\",\n",
        "        fill=True, fill_opacity=0.9,\n",
        "        popup=folium.Popup(\"Speed Camera\", max_width=200)\n",
        "    ).add_to(fg_cams)\n",
        "fg_cams.add_to(m)\n",
        "\n",
        "fg_humps = folium.FeatureGroup(name=\"Devices – Speed Humps\", show=False)\n",
        "for _, r in gdf_humps_4326.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[float(r.geometry.y), float(r.geometry.x)],\n",
        "        radius=3, color=\"#2e7d32\",\n",
        "        fill=True, fill_opacity=0.9,\n",
        "        popup=folium.Popup(\"Speed Hump\", max_width=200)\n",
        "    ).add_to(fg_humps)\n",
        "fg_humps.add_to(m)\n",
        "\n",
        "# 6) Layer: All speeding crashes (UNIFORM style: same size + same color)\n",
        "fg_crashes = folium.FeatureGroup(name=\"All Speeding Crashes (uniform)\", show=True)\n",
        "for _, r in crashes_4326.iterrows():\n",
        "    dt = r[\"FROMDATE\"].date() if \"FROMDATE\" in r and pd.notna(r[\"FROMDATE\"]) else \"\"\n",
        "    sev = int(r.get(\"SEVERITY_SCORE\", 0)) if \"SEVERITY_SCORE\" in r else \"\"\n",
        "\n",
        "    pop = folium.Popup(\n",
        "        f\"Date: {dt}<br>Severity score: {sev}\",\n",
        "        max_width=280\n",
        "    )\n",
        "\n",
        "    folium.CircleMarker(\n",
        "        location=[float(r.geometry.y), float(r.geometry.x)],\n",
        "        radius=2.5,\n",
        "        color=\"#0d6efd\",\n",
        "        fill=True,\n",
        "        fill_color=\"\",\n",
        "        fill_opacity=0.65,\n",
        "        weight=1,\n",
        "        popup=pop\n",
        "    ).add_to(fg_crashes)\n",
        "fg_crashes.add_to(m)\n",
        "\n",
        "# 7) Layer: TOP 10 cluster centroids (variable style by SEVERITY_SUM)\n",
        "if not top10_clusters.empty:\n",
        "    fg_clusters = folium.FeatureGroup(name=\"Top 10 Clusters (by SEVERITY_SUM)\", show=True)\n",
        "\n",
        "    for rank, (_, row) in enumerate(top10_clusters.iterrows(), start=1):\n",
        "        lat = float(row[\"AVG_LAT\"])\n",
        "        lon = float(row[\"AVG_LON\"])\n",
        "        sev = float(row[\"SEVERITY_SUM\"])\n",
        "        n   = int(row[\"N_CRASHES\"])\n",
        "\n",
        "        popup = folium.Popup(\n",
        "            f\"<b>Rank:</b> {rank}<br>\"\n",
        "            f\"<b>Crashes:</b> {n}<br>\"\n",
        "            f\"<b>Severity sum:</b> {int(sev)}<br>\"\n",
        "            f\"<b>Center:</b> {lat:.6f}, {lon:.6f}\",\n",
        "            max_width=360\n",
        "        )\n",
        "\n",
        "        folium.CircleMarker(\n",
        "            location=[lat, lon],\n",
        "            radius=radius_by_sev(sev),\n",
        "            color=cmap(sev),\n",
        "            fill=True,\n",
        "            fill_color=cmap(sev),\n",
        "            fill_opacity=0.9,\n",
        "            weight=2,\n",
        "            popup=popup,\n",
        "            tooltip=f\"Cluster Rank {rank} | Sev {int(sev)} | Cr {n}\"\n",
        "        ).add_to(fg_clusters)\n",
        "\n",
        "    fg_clusters.add_to(m)\n",
        "\n",
        "    cmap.caption = \"Cluster severity (SEVERITY_SUM) — Top 10 only\"\n",
        "    cmap.add_to(m)\n",
        "else:\n",
        "    print(\"[Note] cluster_df not found or empty — skipping top-10 cluster layer.\")\n",
        "\n",
        "folium.LayerControl(collapsed=False).add_to(m)\n",
        "display(m)\n",
        "\n"
      ],
      "metadata": {
        "id": "4X83ti94o7Tq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}