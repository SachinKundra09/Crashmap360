{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Do installations"
      ],
      "metadata": {
        "id": "OGed4ocdRHsL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJhDqDfvQ-pR"
      },
      "outputs": [],
      "source": [
        "# ✅ Run once per runtime\n",
        "!pip -q install geopandas pyogrio shapely rtree folium scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the google drive"
      ],
      "metadata": {
        "id": "RLiocsS8RK-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "ApSlKccORNb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the parameters, paths, and the imports"
      ],
      "metadata": {
        "id": "ptj8lvQGROrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, LineString, MultiLineString\n",
        "\n",
        "# --- Paths ---\n",
        "CRASHES_PATH   = \"/content/drive/My Drive/Crashes_in_DC.csv\"\n",
        "BIKELANES_PATH = \"/content/drive/My Drive/Bicycle_Lanes.geojson\"\n",
        "OUT_DIR        = \"/content/drive/My Drive/outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Analysis window ---\n",
        "DATE_MIN = \"2020-01-01\"\n",
        "DATE_MAX = \"2025-04-30\"\n",
        "\n",
        "# --- Optional filters ---\n",
        "MAR_SCORE_MIN = 100   # set to None to disable\n",
        "\n",
        "# --- Bike-injury columns (will be created if missing) ---\n",
        "fatal_cols = [\"FATAL_BICYCLIST\",\"FATAL_DRIVER\",\"FATAL_PEDESTRIAN\",\"FATALPASSENGER\",\"FATALOTHER\"]\n",
        "major_cols = [\"MAJORINJURIES_BICYCLIST\",\"MAJORINJURIES_DRIVER\",\"MAJORINJURIES_PEDESTRIAN\",\"MAJORINJURIESPASSENGER\",\"MAJORINJURIESOTHER\"]\n",
        "minor_cols = [\"MINORINJURIES_BICYCLIST\",\"MINORINJURIES_DRIVER\",\"MINORINJURIES_PEDESTRIAN\",\"MINORINJURIESPASSENGER\",\"MINORINJURIESOTHER\"]\n",
        "bike_cols  = [\"MAJORINJURIES_BICYCLIST\",\"MINORINJURIES_BICYCLIST\",\"UNKNOWNINJURIES_BICYCLIST\",\"FATAL_BICYCLIST\"]\n"
      ],
      "metadata": {
        "id": "nKKhf28fRTp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load, clean and filter in car accidents"
      ],
      "metadata": {
        "id": "44GModYLRc9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(CRASHES_PATH, low_memory=False)\n",
        "\n",
        "# Coordinates → numeric, drop bad\n",
        "df = df.dropna(subset=[\"LATITUDE\",\"LONGITUDE\"]).copy()\n",
        "df[\"LATITUDE\"]  = pd.to_numeric(df[\"LATITUDE\"],  errors=\"coerce\")\n",
        "df[\"LONGITUDE\"] = pd.to_numeric(df[\"LONGITUDE\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"LATITUDE\",\"LONGITUDE\"]).copy()\n",
        "\n",
        "# Date window\n",
        "df[\"FROMDATE\"] = pd.to_datetime(df[\"FROMDATE\"], errors=\"coerce\")\n",
        "df = df[(df[\"FROMDATE\"] >= DATE_MIN) & (df[\"FROMDATE\"] <= DATE_MAX)].copy()\n",
        "\n",
        "# Optional MAR_SCORE filter\n",
        "if \"MAR_SCORE\" in df.columns and MAR_SCORE_MIN is not None:\n",
        "    df[\"MAR_SCORE\"] = pd.to_numeric(df[\"MAR_SCORE\"], errors=\"coerce\")\n",
        "    df = df[df[\"MAR_SCORE\"] >= MAR_SCORE_MIN].copy()\n",
        "\n",
        "# Ensure injury columns exist & numeric\n",
        "for cols in (fatal_cols, major_cols, minor_cols, bike_cols):\n",
        "    for c in cols:\n",
        "        if c not in df.columns:\n",
        "            df[c] = 0\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# Extract a simple street name from ADDRESS (optional, useful for grouping)\n",
        "def extract_street_name(addr):\n",
        "    if pd.isna(addr): return None\n",
        "    s = str(addr).strip()\n",
        "    if not s or s.lower() in {\"nan\",\"none\",\"null\"}: return None\n",
        "    s = re.sub(r\"(#\\s*\\w+|APT\\s*\\w+|UNIT\\s*\\w+)$\", \"\", s, flags=re.IGNORECASE).strip()\n",
        "    s = re.sub(r\"^\\d+[A-Z]?(?:-\\d+)?\\s+\", \"\", s).strip()      # drop house number\n",
        "    s = re.sub(r\"^(BLOCK OF|BLK|BLOCK)\\s+\", \"\", s, flags=re.IGNORECASE).strip()\n",
        "    s = re.sub(r\"[,\\.;:]+$\", \"\", s).strip()\n",
        "    return s if s else None\n",
        "\n",
        "df[\"street_name\"] = df[\"ADDRESS\"].apply(extract_street_name) if \"ADDRESS\" in df.columns else None\n",
        "\n",
        "print(f\"Crashes after clean/date filters: {len(df):,}\")\n"
      ],
      "metadata": {
        "id": "3PyviJSIRdtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only including accidents that involve bike lanes"
      ],
      "metadata": {
        "id": "NcCiB--tSEzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep rows where any bicyclist injury/fatal is recorded\n",
        "df_bike = df[df[bike_cols].sum(axis=1) > 0].copy()\n",
        "print(f\"Bike-involved crashes: {len(df_bike):,}\")\n"
      ],
      "metadata": {
        "id": "E9NOAnVZSFC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load in the bike lanes as lines"
      ],
      "metadata": {
        "id": "R3EhB9QBSNNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdf_lanes_4326 = gpd.read_file(BIKELANES_PATH)\n",
        "\n",
        "# If layer lacks CRS, assume WGS84\n",
        "if gdf_lanes_4326.crs is None:\n",
        "    gdf_lanes_4326.set_crs(epsg=4326, inplace=True, allow_override=True)\n",
        "\n",
        "# Drop empties / invalids\n",
        "gdf_lanes_4326 = gdf_lanes_4326[\n",
        "    gdf_lanes_4326.geometry.notna() & ~gdf_lanes_4326.geometry.is_empty\n",
        "].copy()\n",
        "\n",
        "# Keep only line-like geometries\n",
        "gdf_lanes_4326 = gdf_lanes_4326[gdf_lanes_4326.geometry.geom_type.isin(\n",
        "    [\"LineString\",\"MultiLineString\"]\n",
        ")].copy()\n",
        "\n",
        "print(f\"Bike lane features (valid line-ish): {len(gdf_lanes_4326):,}\")\n"
      ],
      "metadata": {
        "id": "wG2V02dNSOzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project to meters"
      ],
      "metadata": {
        "id": "Xv4KUUiFiU4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Points (crashes) in 4326\n",
        "gdf_bike_4326 = gpd.GeoDataFrame(\n",
        "    df_bike,\n",
        "    geometry=gpd.points_from_xy(df_bike[\"LONGITUDE\"], df_bike[\"LATITUDE\"]),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "# Project both to 3857 (meters) for later distance work\n",
        "gdf_bike_3857  = gdf_bike_4326.to_crs(epsg=3857)\n",
        "gdf_lanes_3857 = gdf_lanes_4326.to_crs(epsg=3857)\n",
        "\n",
        "# Keep original lat/lon on the point GDF for tooltips/outputs\n",
        "gdf_bike_3857[\"LATITUDE\"]  = gdf_bike_4326[\"LATITUDE\"].values\n",
        "gdf_bike_3857[\"LONGITUDE\"] = gdf_bike_4326[\"LONGITUDE\"].values\n",
        "\n",
        "print(\"CRS (points):\", gdf_bike_3857.crs)\n",
        "print(\"CRS (lanes): \", gdf_lanes_3857.crs)\n",
        "print(f\"Bike points in meters: {len(gdf_bike_3857):,}\")\n",
        "print(f\"Lane features in meters: {len(gdf_lanes_3857):,}\")\n"
      ],
      "metadata": {
        "id": "cs7KJxsyihFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brute force distance calculations"
      ],
      "metadata": {
        "id": "szvGKBWYsZiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 — Pure brute-force nearest bike lane (no prefilters, no classifications)\n",
        "from shapely.geometry import LineString, MultiLineString\n",
        "from shapely.ops import nearest_points\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# 1) Flatten lanes to simple LineStrings (keep original index + optional label)\n",
        "lane_lines = []\n",
        "lane_orig_index = []\n",
        "lane_label_col = None\n",
        "for cand in [\"NAME\",\"STREET\",\"FACILITY\",\"FACILITY_T\",\"TYPE\",\"LABEL\"]:\n",
        "    if cand in gdf_lanes_3857.columns:\n",
        "        lane_label_col = cand\n",
        "        break\n",
        "lane_label_vals = []\n",
        "\n",
        "for idx, geom in zip(gdf_lanes_3857.index, gdf_lanes_3857.geometry):\n",
        "    if geom is None:\n",
        "        continue\n",
        "    if isinstance(geom, LineString):\n",
        "        lane_lines.append(geom)\n",
        "        lane_orig_index.append(idx)\n",
        "        lane_label_vals.append(gdf_lanes_3857.at[idx, lane_label_col] if lane_label_col else None)\n",
        "    elif isinstance(geom, MultiLineString):\n",
        "        for sub in geom.geoms:\n",
        "            lane_lines.append(sub)\n",
        "            lane_orig_index.append(idx)\n",
        "            lane_label_vals.append(gdf_lanes_3857.at[idx, lane_label_col] if lane_label_col else None)\n",
        "\n",
        "print(f\"Flattened lanes: {len(lane_lines)} LineStrings (from {len(gdf_lanes_3857)} features)\")\n",
        "\n",
        "# 2) For each crash: compute exact min distance across ALL lines\n",
        "n = len(gdf_bike_3857)\n",
        "nearest_lane_index = np.empty(n, dtype=object)\n",
        "nearest_lane_label = np.empty(n, dtype=object)\n",
        "nearest_dist_m     = np.empty(n, dtype=float)\n",
        "nearest_onlane_x   = np.empty(n, dtype=float)\n",
        "nearest_onlane_y   = np.empty(n, dtype=float)\n",
        "\n",
        "for i, pt in enumerate(gdf_bike_3857.geometry):\n",
        "    best_d = math.inf\n",
        "    best_j = -1\n",
        "    for j, line in enumerate(lane_lines):\n",
        "        d = pt.distance(line)  # true point-to-line distance (meters)\n",
        "        if d < best_d:\n",
        "            best_d = d\n",
        "            best_j = j\n",
        "\n",
        "    nearest_dist_m[i]     = best_d\n",
        "    nearest_lane_index[i] = lane_orig_index[best_j] if best_j >= 0 else None\n",
        "    nearest_lane_label[i] = lane_label_vals[best_j] if best_j >= 0 else None\n",
        "\n",
        "    if best_j >= 0:\n",
        "        _, q = nearest_points(pt, lane_lines[best_j])\n",
        "        nearest_onlane_x[i] = q.x\n",
        "        nearest_onlane_y[i] = q.y\n",
        "    else:\n",
        "        nearest_onlane_x[i] = np.nan\n",
        "        nearest_onlane_y[i] = np.nan\n",
        "\n",
        "# 3) Assemble results\n",
        "gdf_with_dist = gdf_bike_3857.copy()\n",
        "gdf_with_dist[\"nearest_lane_index\"] = nearest_lane_index\n",
        "gdf_with_dist[\"nearest_lane_label\"] = nearest_lane_label\n",
        "gdf_with_dist[\"dist_to_lane_m\"]     = nearest_dist_m\n",
        "gdf_with_dist[\"nearest_onlane_x\"]   = nearest_onlane_x\n",
        "gdf_with_dist[\"nearest_onlane_y\"]   = nearest_onlane_y\n",
        "\n",
        "print(\"\\nDistance summary (meters):\")\n",
        "print(gdf_with_dist[\"dist_to_lane_m\"].describe())\n",
        "print(\"\\nSample:\")\n",
        "print(gdf_with_dist[[\"dist_to_lane_m\",\"nearest_lane_label\"]].head())\n"
      ],
      "metadata": {
        "id": "8Z6iogAOsbSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter out the crashes that are within the threshold"
      ],
      "metadata": {
        "id": "4e3YbK1X8dcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== TEMPORARY DISTANCE THRESHOLD (easy to adjust) ====\n",
        "FAR_THRESH_M = 100   # <-- change this anytime (e.g., 50, 150)\n",
        "\n",
        "# Keep only crashes farther than the threshold from any bike lane\n",
        "far_df = gdf_with_dist[gdf_with_dist[\"dist_to_lane_m\"] > FAR_THRESH_M].copy()\n",
        "\n",
        "print(f\"Far-from-lane crashes (> {FAR_THRESH_M} m): {len(far_df)} / {len(gdf_with_dist)} total\")\n"
      ],
      "metadata": {
        "id": "Yvv6aFCw8or1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect each of the car accidents to streets using the DDOT's lines."
      ],
      "metadata": {
        "id": "KHo5KGC28pCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters for DDOT"
      ],
      "metadata": {
        "id": "dDP8YJQ3-zT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, pandas as pd, geopandas as gpd\n",
        "\n",
        "# Your roadway layer (DDOT centerlines)\n",
        "SPEED_GEOJSON_PATH = \"/content/drive/My Drive/Roadway_SubBlock.geojson\"\n",
        "\n",
        "# CRS + snap threshold (easy to tweak)\n",
        "CRS_WGS84     = 4326\n",
        "CRS_METERS    = 3857\n",
        "MAX_LATERAL_M = 30   # <-- change this later if needed\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XLgAM0Oh-1OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load DDOT lines"
      ],
      "metadata": {
        "id": "CMqX8oH7-269"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ddot_lines = gpd.read_file(SPEED_GEOJSON_PATH)\n",
        "\n",
        "# Ensure CRS, drop empties, project to meters\n",
        "if ddot_lines.crs is None:\n",
        "    ddot_lines.set_crs(epsg=CRS_WGS84, inplace=True, allow_override=True)\n",
        "ddot_lines = ddot_lines[ddot_lines.geometry.notna() & ~ddot_lines.geometry.is_empty].copy()\n",
        "ddot_lines = ddot_lines.to_crs(epsg=CRS_METERS)\n",
        "\n",
        "# Ensure a 'street_base' string column (derive from a name-like field if present)\n",
        "name_col = None\n",
        "for c in [\"STNAME\",\"NAME\",\"FULLNAME\",\"ROADNAME\",\"STREET\",\"LABEL\"]:\n",
        "    if c in ddot_lines.columns:\n",
        "        name_col = c\n",
        "        break\n",
        "\n",
        "def mk_street_base(s):\n",
        "    if pd.isna(s): return \"\"\n",
        "    s = str(s).strip().upper()\n",
        "    s = re.sub(r\"^\\d+[A-Z]?(?:-\\d+)?\\s+\", \"\", s)  # drop leading numbers\n",
        "    s = re.sub(r\"[,\\.;:]+$\", \"\", s)               # drop trailing punctuation\n",
        "    s = re.sub(r\"\\s+\", \" \", s)                    # collapse spaces\n",
        "    return s\n",
        "\n",
        "if \"street_base\" not in ddot_lines.columns:\n",
        "    ddot_lines[\"street_base\"] = ddot_lines[name_col].apply(mk_street_base) if name_col else \"\"\n",
        "\n",
        "print(\"DDOT lines ready → features:\", len(ddot_lines), \"| CRS:\", ddot_lines.crs)\n",
        "\n"
      ],
      "metadata": {
        "id": "dGWryWee_UF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign the car accidents to the DDOT lines"
      ],
      "metadata": {
        "id": "qSKOQ-5aA5lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) ASSIGN FAR-FROM-LANE BIKE CRASHES — map to nearest DDOT line, inherit street_base\n",
        "# Expects: far_df (already filtered to > FAR_THRESH_M) with geometry in meters\n",
        "\n",
        "# Ensure DDOT CRS matches far_df\n",
        "if ddot_lines.crs != far_df.crs:\n",
        "    ddot_lines = ddot_lines.to_crs(far_df.crs)\n",
        "\n",
        "# Only keep needed DDOT fields\n",
        "ddot_min = ddot_lines[[\"street_base\", \"geometry\"]].copy()\n",
        "\n",
        "cr2line = (\n",
        "    gpd.sjoin_nearest(far_df, ddot_min, how=\"left\", distance_col=\"cr_dist_to_line_m\")\n",
        "      .rename(columns={\"index_right\": \"ddot_idx\"})\n",
        "      .reset_index()\n",
        "      .rename(columns={\"index\": \"crash_index\"})\n",
        ")\n",
        "\n",
        "cr2line[\"cr_valid_line\"] = cr2line[\"cr_dist_to_line_m\"] <= MAX_LATERAL_M\n",
        "cr2line[\"street_base\"]   = cr2line[\"street_base\"].fillna(\"\").astype(str)\n",
        "\n",
        "print(\"Far crashes valid to street:\", int(cr2line[\"cr_valid_line\"].sum()), \"/\", len(cr2line))\n"
      ],
      "metadata": {
        "id": "ACynKZtOA4Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete Linkage"
      ],
      "metadata": {
        "id": "219pcepJJeJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL — Complete-linkage clustering (per street), keeping compact hotspots\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "\n",
        "# Params (easy to tweak)\n",
        "CLUSTER_DIST_M = 50   # max diameter of a cluster (meters)\n",
        "MIN_POINTS     = 3    # minimum crashes per cluster to keep\n",
        "\n",
        "# Work on valid snaps only\n",
        "far_valid = cr2line[cr2line[\"cr_valid_line\"]].copy()\n",
        "if far_valid.empty:\n",
        "    print(\"No far-from-lane crashes valid to a street. Nothing to cluster.\")\n",
        "    clustered = far_valid.copy()\n",
        "    clustered[\"cluster_id\"] = -1\n",
        "else:\n",
        "    # Calculate a simple severity score if ingredients exist\n",
        "    fatal_cols = [\"FATAL_BICYCLIST\",\"FATAL_DRIVER\",\"FATAL_PEDESTRIAN\",\"FATALPASSENGER\",\"FATALOTHER\"]\n",
        "    major_cols = [\"MAJORINJURIES_BICYCLIST\",\"MAJORINJURIES_DRIVER\",\"MAJORINJURIES_PEDESTRIAN\",\"MAJORINJURIESPASSENGER\",\"MAJORINJURIESOTHER\"]\n",
        "    minor_cols = [\"MINORINJURIES_BICYCLIST\",\"MINORINJURIES_DRIVER\",\"MINORINJURIES_PEDESTRIAN\",\"MINORINJURIESPASSENGER\",\"MINORINJURIESOTHER\"]\n",
        "    for cols in (fatal_cols, major_cols, minor_cols):\n",
        "        for c in cols:\n",
        "            if c not in far_valid.columns:\n",
        "                far_valid[c] = 0\n",
        "            far_valid[c] = pd.to_numeric(far_valid[c], errors=\"coerce\").fillna(0)\n",
        "    far_valid[\"SEVERITY_SCORE\"] = (\n",
        "          7 * far_valid[fatal_cols].sum(axis=1)\n",
        "        + 4 * far_valid[major_cols].sum(axis=1)\n",
        "        + 1 * far_valid[minor_cols].sum(axis=1)\n",
        "    )\n",
        "\n",
        "    parts = []\n",
        "    global_id = 0\n",
        "    for street, grp in far_valid.groupby(far_valid[\"street_base\"].fillna(\"\").astype(str)):\n",
        "        if len(grp) < MIN_POINTS:\n",
        "            continue\n",
        "        XY = np.c_[grp.geometry.x.values, grp.geometry.y.values]\n",
        "        model = AgglomerativeClustering(\n",
        "            n_clusters=None,\n",
        "            distance_threshold=CLUSTER_DIST_M,\n",
        "            linkage=\"complete\"\n",
        "        )\n",
        "        labels = model.fit_predict(XY)\n",
        "        grp = grp.copy()\n",
        "        # keep only real clusters (size >= MIN_POINTS)\n",
        "        grp[\"local_label\"] = labels\n",
        "        counts = grp[\"local_label\"].value_counts()\n",
        "        keep = counts[counts >= MIN_POINTS].index\n",
        "        grp = grp[grp[\"local_label\"].isin(keep)].copy()\n",
        "        if grp.empty:\n",
        "            continue\n",
        "        # make globally unique cluster ids\n",
        "        label_map = {lab: (global_id + i) for i, lab in enumerate(sorted(keep))}\n",
        "        grp[\"cluster_id\"] = grp[\"local_label\"].map(label_map)\n",
        "        global_id += len(keep)\n",
        "        parts.append(grp.drop(columns=[\"local_label\"]))\n",
        "\n",
        "    clustered = gpd.GeoDataFrame(pd.concat(parts, axis=0), crs=far_valid.crs) if parts else gpd.GeoDataFrame(geometry=[], crs=far_valid.crs)\n",
        "\n",
        "print(f\"Clustered points kept: {0 if clustered.empty else len(clustered)}\")\n"
      ],
      "metadata": {
        "id": "g-2MDHn4JuAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Table"
      ],
      "metadata": {
        "id": "Et0tpyR3Jkn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Summarize valid clusters (structured exactly like sample) ===\n",
        "print(\"\\nSummarizing valid clusters...\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "\n",
        "# Safety checks\n",
        "if clustered is None or clustered.empty:\n",
        "    print(\"No clustered crashes available. (clustered is empty)\")\n",
        "    cluster_simple = pd.DataFrame(columns=[\"RANK\",\"N_CRASHES\",\"SEVERITY_SUM\",\"AVG_LON\",\"AVG_LAT\"])\n",
        "    display(cluster_simple)\n",
        "else:\n",
        "    fc = clustered.copy()\n",
        "\n",
        "    # Require cluster_id\n",
        "    if \"cluster_id\" not in fc.columns:\n",
        "        raise ValueError(\"Expected 'cluster_id' in clustered, but it was not found.\")\n",
        "\n",
        "    # -------- 1. Identify all injury columns --------\n",
        "    injury_cols = [\n",
        "        'MAJORINJURIES_BICYCLIST','MINORINJURIES_BICYCLIST','UNKNOWNINJURIES_BICYCLIST','FATAL_BICYCLIST',\n",
        "        'MAJORINJURIES_DRIVER','MINORINJURIES_DRIVER','UNKNOWNINJURIES_DRIVER','FATAL_DRIVER',\n",
        "        'MAJORINJURIES_PEDESTRIAN','MINORINJURIES_PEDESTRIAN','UNKNOWNINJURIES_PEDESTRIAN','FATAL_PEDESTRIAN',\n",
        "        'FATALPASSENGER','MAJORINJURIESPASSENGER','MINORINJURIESPASSENGER','UNKNOWNINJURIESPASSENGER',\n",
        "        'MAJORINJURIESOTHER','MINORINJURIESOTHER','UNKNOWNINJURIESOTHER','FATALOTHER'\n",
        "    ]\n",
        "\n",
        "    # Keep only those that actually exist\n",
        "    injury_cols = [c for c in injury_cols if c in fc.columns]\n",
        "\n",
        "    # If none exist, every crash still counts at least 1\n",
        "    if len(injury_cols) == 0:\n",
        "        fc[\"CRASH_SEVERITY\"] = 1\n",
        "    else:\n",
        "        # Coerce to numeric\n",
        "        for c in injury_cols:\n",
        "            fc[c] = pd.to_numeric(fc[c], errors=\"coerce\").fillna(0)\n",
        "\n",
        "        # Categorize\n",
        "        fatal_cols  = [c for c in injury_cols if \"FATAL\" in c.upper()]\n",
        "        major_cols  = [c for c in injury_cols if \"MAJOR\" in c.upper()]\n",
        "        minor_cols  = [c for c in injury_cols if \"MINOR\" in c.upper()]\n",
        "\n",
        "        fatal_any = fc[fatal_cols].sum(axis=1) > 0 if fatal_cols else np.zeros(len(fc), dtype=bool)\n",
        "        major_any = fc[major_cols].sum(axis=1) > 0 if major_cols else np.zeros(len(fc), dtype=bool)\n",
        "        minor_any = fc[minor_cols].sum(axis=1) > 0 if minor_cols else np.zeros(len(fc), dtype=bool)\n",
        "\n",
        "        # Priority: fatal > major > minor; default=1 (every crash counts)\n",
        "        fc[\"CRASH_SEVERITY\"] = np.select(\n",
        "            [fatal_any, major_any, minor_any],\n",
        "            [7,         4,         1],\n",
        "            default=1\n",
        "        )\n",
        "\n",
        "    # -------- 2. Get lon/lat for each crash point --------\n",
        "    # clustered is likely in a projected CRS (meters). Convert to EPSG:4326 for lon/lat summaries.\n",
        "    fc_ll = fc.to_crs(4326).copy()\n",
        "    fc_ll[\"LON\"] = fc_ll.geometry.x\n",
        "    fc_ll[\"LAT\"] = fc_ll.geometry.y\n",
        "\n",
        "    # -------- 3. Summarize each cluster (matching your sample structure) --------\n",
        "    grp = fc_ll.groupby(\"cluster_id\", dropna=False)\n",
        "\n",
        "    cluster_summary_df = grp.agg(\n",
        "        N_CRASHES=(\"cluster_id\", \"size\"),\n",
        "        SEVERITY_SUM=(\"CRASH_SEVERITY\", \"sum\"),\n",
        "        AVG_LON=(\"LON\", \"mean\"),\n",
        "        AVG_LAT=(\"LAT\", \"mean\")\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    # Sort by severity then crashes (desc), add rank, and keep the exact column order\n",
        "    cluster_simple = cluster_summary_df.sort_values(\n",
        "        [\"SEVERITY_SUM\", \"N_CRASHES\"], ascending=[False, False]\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    cluster_simple.insert(0, \"RANK\", np.arange(1, len(cluster_simple) + 1))\n",
        "\n",
        "    print(f\"Final clusters summarized: {len(cluster_simple)}\")\n",
        "    print(\"\\nTop 10 clusters (RANK, N_CRASHES, SEVERITY_SUM, AVG_LON, AVG_LAT):\")\n",
        "    display(cluster_simple.head(10))\n"
      ],
      "metadata": {
        "id": "KvFC5dwjZxkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL — Folium map of far crashes + TOP 10 clusters (fixed RANK + robust)\n",
        "import folium\n",
        "from branca.colormap import linear\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --------------------------\n",
        "# 1) Prep layers to EPSG:4326\n",
        "# --------------------------\n",
        "far_points_ll = cr2line[cr2line[\"cr_valid_line\"]].to_crs(4326) if \"cr_valid_line\" in cr2line.columns else far_df.to_crs(4326)\n",
        "\n",
        "# Keep only geometry for DDOT lines to avoid Timestamp serialization errors\n",
        "ddot_ll_min = ddot_lines[[\"geometry\"]].to_crs(4326).copy()\n",
        "\n",
        "# --------------------------\n",
        "# 2) Initialize map\n",
        "# --------------------------\n",
        "m = folium.Map(location=[38.9072, -77.0369], zoom_start=12, tiles=\"cartodbpositron\")\n",
        "\n",
        "# DDOT roadways\n",
        "folium.GeoJson(\n",
        "    data=ddot_ll_min.__geo_interface__,\n",
        "    name=\"DDOT Roadways\",\n",
        "    style_function=lambda _: {\"color\": \"#6c757d\", \"weight\": 1, \"opacity\": 0.5},\n",
        ").add_to(m)\n",
        "\n",
        "# --------------------------\n",
        "# 3) All far crashes (uniform style)\n",
        "# --------------------------\n",
        "fg_far = folium.FeatureGroup(name=\"Far-from-lane crashes (uniform)\", show=True)\n",
        "for _, r in far_points_ll.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[float(r.geometry.y), float(r.geometry.x)],\n",
        "        radius=3,\n",
        "        color=\"#0d6efd\",\n",
        "        fill=True,\n",
        "        fill_color=\"#0d6efd\",\n",
        "        fill_opacity=0.7,\n",
        "        opacity=1.0,\n",
        "        weight=0\n",
        "    ).add_to(fg_far)\n",
        "fg_far.add_to(m)\n",
        "\n",
        "# --------------------------\n",
        "# 4) Choose the right cluster summary DF to map\n",
        "#    Prefer cluster_simple (has RANK), else fall back to cluster_summary_df\n",
        "# --------------------------\n",
        "cluster_df = None\n",
        "\n",
        "if \"cluster_simple\" in globals() and isinstance(cluster_simple, pd.DataFrame) and not cluster_simple.empty:\n",
        "    cluster_df = cluster_simple.copy()\n",
        "elif \"cluster_summary_df\" in globals() and isinstance(cluster_summary_df, pd.DataFrame) and not cluster_summary_df.empty:\n",
        "    cluster_df = cluster_summary_df.copy()\n",
        "    # If this DF doesn't have RANK, create it after sorting\n",
        "    # Ensure it has the core columns needed\n",
        "    needed = {\"N_CRASHES\", \"SEVERITY_SUM\", \"AVG_LON\", \"AVG_LAT\"}\n",
        "    if not needed.issubset(set(cluster_df.columns)):\n",
        "        raise ValueError(f\"cluster_summary_df is missing required columns: {needed - set(cluster_df.columns)}\")\n",
        "\n",
        "    cluster_df = cluster_df.sort_values([\"SEVERITY_SUM\", \"N_CRASHES\"], ascending=[False, False]).reset_index(drop=True)\n",
        "    cluster_df.insert(0, \"RANK\", np.arange(1, len(cluster_df) + 1))\n",
        "\n",
        "# --------------------------\n",
        "# 5) Plot TOP 10 clusters only (variable size + color)\n",
        "# --------------------------\n",
        "if cluster_df is not None and not cluster_df.empty:\n",
        "    top10 = cluster_df.sort_values([\"SEVERITY_SUM\", \"N_CRASHES\"], ascending=[False, False]).head(10).reset_index(drop=True)\n",
        "\n",
        "    vmin = float(top10[\"SEVERITY_SUM\"].min())\n",
        "    vmax = float(top10[\"SEVERITY_SUM\"].max())\n",
        "    if vmin == vmax:\n",
        "        vmin = 0.0  # avoid divide-by-zero in scaling\n",
        "    cmap = linear.Reds_09.scale(vmin, vmax)\n",
        "\n",
        "    fg_cl = folium.FeatureGroup(name=\"Top 10 clusters (severity)\", show=True)\n",
        "\n",
        "    for _, row in top10.iterrows():\n",
        "        sev = float(row[\"SEVERITY_SUM\"])\n",
        "        n_crashes = int(row[\"N_CRASHES\"])\n",
        "        rank = int(row[\"RANK\"]) if \"RANK\" in top10.columns else None\n",
        "\n",
        "        # Radius scaled by severity (bounded)\n",
        "        if vmax == vmin:\n",
        "            radius = 10\n",
        "        else:\n",
        "            radius = 8 + 22 * (sev - vmin) / (vmax - vmin)\n",
        "\n",
        "        popup = (\n",
        "            (f\"<b>Rank:</b> {rank}<br>\" if rank is not None else \"\") +\n",
        "            f\"<b>Crashes:</b> {n_crashes}<br>\"\n",
        "            f\"<b>Severity sum:</b> {int(sev)}<br>\"\n",
        "            f\"<b>Center lon:</b> {float(row['AVG_LON']):.5f}<br>\"\n",
        "            f\"<b>Center lat:</b> {float(row['AVG_LAT']):.5f}\"\n",
        "        )\n",
        "\n",
        "        folium.CircleMarker(\n",
        "            location=[float(row[\"AVG_LAT\"]), float(row[\"AVG_LON\"])],\n",
        "            radius=float(radius),\n",
        "            color=cmap(sev),\n",
        "            fill=True,\n",
        "            fill_color=cmap(sev),\n",
        "            fill_opacity=0.9,\n",
        "            opacity=1.0,\n",
        "            weight=2,\n",
        "            popup=folium.Popup(popup, max_width=360),\n",
        "            tooltip=(f\"Rank {rank} | Sev {int(sev)} | Cr {n_crashes}\" if rank is not None else f\"Sev {int(sev)} | Cr {n_crashes}\")\n",
        "        ).add_to(fg_cl)\n",
        "\n",
        "    fg_cl.add_to(m)\n",
        "    cmap.caption = \"Top 10 cluster severity (SEVERITY_SUM)\"\n",
        "    cmap.add_to(m)\n",
        "else:\n",
        "    print(\"No cluster summary available to plot (cluster_simple / cluster_summary_df missing or empty).\")\n",
        "\n",
        "# --------------------------\n",
        "# 6) Layer control\n",
        "# --------------------------\n",
        "folium.LayerControl(collapsed=False).add_to(m)\n",
        "m\n"
      ],
      "metadata": {
        "id": "O8nMlch2Jrpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "map"
      ],
      "metadata": {
        "id": "RO20lXqCJsTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL — Folium map of far crashes + complete-linkage cluster centroids (clean + fixed)\n",
        "import folium\n",
        "from branca.colormap import linear\n",
        "\n",
        "# Prep layers to 4326\n",
        "far_points_ll = cr2line[cr2line[\"cr_valid_line\"]].to_crs(4326) if \"cr_valid_line\" in cr2line.columns else far_df.to_crs(4326)\n",
        "\n",
        "# ⚠️ Keep only geometry for DDOT lines to avoid Timestamp serialization errors\n",
        "ddot_ll_min = ddot_lines[[\"geometry\"]].to_crs(4326).copy()\n",
        "\n",
        "# Map init (DC)\n",
        "m = folium.Map(location=[38.9072, -77.0369], zoom_start=12, tiles=\"cartodbpositron\")\n",
        "\n",
        "# DDOT lines overlay (geometry only)\n",
        "folium.GeoJson(\n",
        "    data=ddot_ll_min.__geo_interface__,\n",
        "    name=\"DDOT Roadways\",\n",
        "    style_function=lambda _: {\"color\": \"#6c757d\", \"weight\": 1, \"opacity\": 0.5},\n",
        ").add_to(m)\n",
        "\n",
        "# Far-from-lane crashes layer (points)\n",
        "fg_far = folium.FeatureGroup(name=\"Far-from-lane crashes (>100 m)\", show=True)\n",
        "for _, r in far_points_ll.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[float(r.geometry.y), float(r.geometry.x)],\n",
        "        radius=3,\n",
        "        color=\"#0d6efd\",\n",
        "        fill=True,\n",
        "        fill_opacity=0.7\n",
        "    ).add_to(fg_far)\n",
        "fg_far.add_to(m)\n",
        "\n",
        "# Cluster centroids layer (if available)\n",
        "if not street_summary.empty:\n",
        "    vmin = float(street_summary[\"severity_sum\"].min())\n",
        "    vmax = float(street_summary[\"severity_sum\"].max())\n",
        "    if vmin == vmax:\n",
        "        vmin = 0.0\n",
        "    cmap = linear.Reds_09.scale(vmin, vmax)\n",
        "\n",
        "    fg_cl = folium.FeatureGroup(name=\"Clusters (complete linkage)\", show=True)\n",
        "    for _, row in street_summary.iterrows():\n",
        "        sev = float(row[\"severity_sum\"])\n",
        "        radius = 6 if vmax == vmin else 6 + 20 * (sev - vmin) / (vmax - vmin)\n",
        "        popup = (\n",
        "            f\"<b>Street:</b> {row['street_base']}<br>\"\n",
        "            f\"<b>Cluster ID:</b> {int(row['cluster_id'])}<br>\"\n",
        "            f\"<b>Crashes:</b> {int(row['crashes'])}<br>\"\n",
        "            f\"<b>Severity sum:</b> {int(sev)}<br>\"\n",
        "            f\"<b>Mean dist to street (m):</b> {float(row['mean_dist_m']):.1f}<br>\"\n",
        "            f\"<b>Median dist (m):</b> {float(row['med_dist_m']):.1f}\"\n",
        "        )\n",
        "        folium.CircleMarker(\n",
        "            location=[float(row[\"avg_lat\"]), float(row[\"avg_lon\"])],\n",
        "            radius=radius,\n",
        "            color=cmap(sev),\n",
        "            fill=True,\n",
        "            fill_color=cmap(sev),\n",
        "            fill_opacity=0.9,\n",
        "            popup=folium.Popup(popup, max_width=360),\n",
        "            tooltip=f\"{row['street_base']} | Cl {int(row['cluster_id'])}\"\n",
        "        ).add_to(fg_cl)\n",
        "\n",
        "    fg_cl.add_to(m)\n",
        "    cmap.caption = \"Cluster severity (sum)\"\n",
        "    cmap.add_to(m)\n",
        "\n",
        "folium.LayerControl(collapsed=False).add_to(m)\n",
        "m\n",
        "\n"
      ],
      "metadata": {
        "id": "KalIfmuQJtfm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}